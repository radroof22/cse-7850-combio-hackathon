{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create mutated sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "# get the sequence\n",
    "seq = open('sequence.fasta', 'r').read()\n",
    "seq = seq.split(\"\\n\")[1]\n",
    "\n",
    "# create each mutated sequence using the info\n",
    "sequences = []\n",
    "for i in df['mutant']:\n",
    "    ind = int(i[1:-1])\n",
    "    tmp = seq[:ind] + i[-1] + seq[ind+1:]\n",
    "    sequences.append(tmp)\n",
    "df['sequence'] = sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LENGTH = len(seq)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare One_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the amino acids\n",
    "amino_acids = \"ACDEFGHIKLMNPQRSTVWXY\"  # 20 standard amino acids\n",
    "\n",
    "# Create a dictionary to map amino acids to indices\n",
    "aa_dict = {aa: idx for idx, aa in enumerate(amino_acids)}\n",
    "\n",
    "# Function to convert sequence to one-hot encoding\n",
    "def seq_to_one_hot(sequence):\n",
    "    one_hot = np.zeros((len(sequence), len(amino_acids)))\n",
    "    for i, aa in enumerate(sequence):\n",
    "        if aa in aa_dict:\n",
    "            one_hot[i, aa_dict[aa]] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the sequences into one-hot encodings\n",
    "df['one_hot_sequence'] = df['sequence'].apply(lambda seq: seq_to_one_hot(seq).tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mutant</th>\n",
       "      <th>DMS_score</th>\n",
       "      <th>sequence</th>\n",
       "      <th>one_hot_sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M0Y</td>\n",
       "      <td>0.2730</td>\n",
       "      <td>YVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M0W</td>\n",
       "      <td>0.2857</td>\n",
       "      <td>WVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M0V</td>\n",
       "      <td>0.2153</td>\n",
       "      <td>VVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M0T</td>\n",
       "      <td>0.3122</td>\n",
       "      <td>TVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M0S</td>\n",
       "      <td>0.2180</td>\n",
       "      <td>SVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  mutant  DMS_score                                           sequence  \\\n",
       "0    M0Y     0.2730  YVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...   \n",
       "1    M0W     0.2857  WVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...   \n",
       "2    M0V     0.2153  VVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...   \n",
       "3    M0T     0.3122  TVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...   \n",
       "4    M0S     0.2180  SVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...   \n",
       "\n",
       "                                    one_hot_sequence  \n",
       "0  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "1  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "2  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "3  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "4  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare K_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "def get_data(df):\n",
    "    X = np.array(df['one_hot_sequence'].tolist())  # One-hot encoded sequences\n",
    "    X = X.transpose(0, 2, 1) # shape expected (N, C_in, L)\n",
    "    y = np.array(df['DMS_score'].tolist())  # DMS scores (target)\n",
    "    return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AminoAcidCNN(\n",
      "  (conv1): Conv1d(21, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (conv2): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (conv3): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=10496, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define the CNN model for input shape (batch_size, 656, 21)\n",
    "class AminoAcidCNN(nn.Module):\n",
    "    def __init__(self, seq_length, num_amino_acids):\n",
    "        super(AminoAcidCNN, self).__init__()\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=num_amino_acids, out_channels=32, kernel_size=3, padding=1)  # 32 filters\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1)  # 64 filters\n",
    "        self.conv3 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)  # 128 filters\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Fully connected layers\n",
    "        # After applying 3 convolution layers and pooling, the sequence length will be reduced by a factor of 8\n",
    "        self.fc1 = nn.Linear(128 * (seq_length // 8), 64)  # Adjust the output dimension based on the input length\n",
    "        self.fc2 = nn.Linear(64, 1)  # Single output for fitness score\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through convolutional layers\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = self.pool(torch.relu(self.conv3(x)))\n",
    "\n",
    "        # Flatten the tensor for the fully connected layers\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)  # Output layer (fitness score)\n",
    "        return x\n",
    "\n",
    "# Example usage:\n",
    "num_amino_acids = len(amino_acids)  # For standard amino acids (A, C, D, E, F, G, H, I, K, L, M, N, P, Q, R, S, T, V, W, Y)\n",
    "\n",
    "# Create the model\n",
    "cnn_model = AminoAcidCNN(SEQ_LENGTH, num_amino_acids)\n",
    "\n",
    "# Print the model summary (optional)\n",
    "print(cnn_model)\n",
    "\n",
    "# Example input: batch size of 32, sequence length 100, and 20\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "K = 5\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_data(df)\n",
    "num_samples, seq_length, num_amino_acids = X.shape\n",
    "\n",
    "kfold = KFold(n_splits=K, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1/5\n",
      "Epoch [1/100], Loss: 0.2884\n",
      "Epoch [2/100], Loss: 0.0434\n",
      "Epoch [3/100], Loss: 0.1611\n",
      "Epoch [4/100], Loss: 0.0376\n",
      "Epoch [5/100], Loss: 0.2664\n",
      "Epoch [6/100], Loss: 0.0944\n",
      "Epoch [7/100], Loss: 0.1452\n",
      "Epoch [8/100], Loss: 0.1012\n",
      "Epoch [9/100], Loss: 0.0611\n",
      "Epoch [10/100], Loss: 0.0726\n",
      "Epoch [11/100], Loss: 0.0682\n",
      "Epoch [12/100], Loss: 0.0673\n",
      "Epoch [13/100], Loss: 0.0730\n",
      "Epoch [14/100], Loss: 0.0629\n",
      "Epoch [15/100], Loss: 0.0829\n",
      "Epoch [16/100], Loss: 0.0600\n",
      "Epoch [17/100], Loss: 0.0972\n",
      "Epoch [18/100], Loss: 0.0810\n",
      "Epoch [19/100], Loss: 0.0679\n",
      "Epoch [20/100], Loss: 0.0752\n",
      "Epoch [21/100], Loss: 0.0660\n",
      "Epoch [22/100], Loss: 0.0708\n",
      "Epoch [23/100], Loss: 0.0678\n",
      "Epoch [24/100], Loss: 0.0689\n",
      "Epoch [25/100], Loss: 0.0690\n",
      "Epoch [26/100], Loss: 0.0678\n",
      "Epoch [27/100], Loss: 0.0702\n",
      "Epoch [28/100], Loss: 0.0666\n",
      "Epoch [29/100], Loss: 0.0725\n",
      "Epoch [30/100], Loss: 0.0644\n",
      "Epoch [31/100], Loss: 0.0786\n",
      "Epoch [32/100], Loss: 0.0619\n",
      "Epoch [33/100], Loss: 0.0914\n",
      "Epoch [34/100], Loss: 0.0709\n",
      "Epoch [35/100], Loss: 0.0843\n",
      "Epoch [36/100], Loss: 0.0801\n",
      "Epoch [37/100], Loss: 0.0670\n",
      "Epoch [38/100], Loss: 0.0732\n",
      "Epoch [39/100], Loss: 0.0671\n",
      "Epoch [40/100], Loss: 0.0700\n",
      "Epoch [41/100], Loss: 0.0684\n",
      "Epoch [42/100], Loss: 0.0686\n",
      "Epoch [43/100], Loss: 0.0694\n",
      "Epoch [44/100], Loss: 0.0677\n",
      "Epoch [45/100], Loss: 0.0704\n",
      "Epoch [46/100], Loss: 0.0669\n",
      "Epoch [47/100], Loss: 0.0721\n",
      "Epoch [48/100], Loss: 0.0656\n",
      "Epoch [49/100], Loss: 0.0756\n",
      "Epoch [50/100], Loss: 0.0645\n",
      "Epoch [51/100], Loss: 0.0816\n",
      "Epoch [52/100], Loss: 0.0673\n",
      "Epoch [53/100], Loss: 0.0826\n",
      "Epoch [54/100], Loss: 0.0744\n",
      "Epoch [55/100], Loss: 0.0723\n",
      "Epoch [56/100], Loss: 0.0727\n",
      "Epoch [57/100], Loss: 0.0696\n",
      "Epoch [58/100], Loss: 0.0705\n",
      "Epoch [59/100], Loss: 0.0689\n",
      "Epoch [60/100], Loss: 0.0693\n",
      "Epoch [61/100], Loss: 0.0693\n",
      "Epoch [62/100], Loss: 0.0684\n",
      "Epoch [63/100], Loss: 0.0698\n",
      "Epoch [64/100], Loss: 0.0678\n",
      "Epoch [65/100], Loss: 0.0706\n",
      "Epoch [66/100], Loss: 0.0672\n",
      "Epoch [67/100], Loss: 0.0718\n",
      "Epoch [68/100], Loss: 0.0666\n",
      "Epoch [69/100], Loss: 0.0737\n",
      "Epoch [70/100], Loss: 0.0663\n",
      "Epoch [71/100], Loss: 0.0761\n",
      "Epoch [72/100], Loss: 0.0673\n",
      "Epoch [73/100], Loss: 0.0768\n",
      "Epoch [74/100], Loss: 0.0700\n",
      "Epoch [75/100], Loss: 0.0738\n",
      "Epoch [76/100], Loss: 0.0711\n",
      "Epoch [77/100], Loss: 0.0706\n",
      "Epoch [78/100], Loss: 0.0704\n",
      "Epoch [79/100], Loss: 0.0694\n",
      "Epoch [80/100], Loss: 0.0694\n",
      "Epoch [81/100], Loss: 0.0694\n",
      "Epoch [82/100], Loss: 0.0687\n",
      "Epoch [83/100], Loss: 0.0697\n",
      "Epoch [84/100], Loss: 0.0681\n",
      "Epoch [85/100], Loss: 0.0702\n",
      "Epoch [86/100], Loss: 0.0677\n",
      "Epoch [87/100], Loss: 0.0710\n",
      "Epoch [88/100], Loss: 0.0673\n",
      "Epoch [89/100], Loss: 0.0721\n",
      "Epoch [90/100], Loss: 0.0670\n",
      "Epoch [91/100], Loss: 0.0735\n",
      "Epoch [92/100], Loss: 0.0672\n",
      "Epoch [93/100], Loss: 0.0745\n",
      "Epoch [94/100], Loss: 0.0683\n",
      "Epoch [95/100], Loss: 0.0739\n",
      "Epoch [96/100], Loss: 0.0697\n",
      "Epoch [97/100], Loss: 0.0718\n",
      "Epoch [98/100], Loss: 0.0700\n",
      "Epoch [99/100], Loss: 0.0702\n",
      "Epoch [100/100], Loss: 0.0696\n",
      "Validation Loss for fold 1: 0.0495\n",
      "Training fold 2/5\n",
      "Epoch [1/100], Loss: 0.2917\n",
      "Epoch [2/100], Loss: 0.0578\n",
      "Epoch [3/100], Loss: 0.1805\n",
      "Epoch [4/100], Loss: 0.0950\n",
      "Epoch [5/100], Loss: 0.0422\n",
      "Epoch [6/100], Loss: 0.2292\n",
      "Epoch [7/100], Loss: 0.0526\n",
      "Epoch [8/100], Loss: 0.2552\n",
      "Epoch [9/100], Loss: 0.1460\n",
      "Epoch [10/100], Loss: 0.1054\n",
      "Epoch [11/100], Loss: 0.0816\n",
      "Epoch [12/100], Loss: 0.1098\n",
      "Epoch [13/100], Loss: 0.1129\n",
      "Epoch [14/100], Loss: 0.0621\n",
      "Epoch [15/100], Loss: 0.0895\n",
      "Epoch [16/100], Loss: 0.0767\n",
      "Epoch [17/100], Loss: 0.0810\n",
      "Epoch [18/100], Loss: 0.0798\n",
      "Epoch [19/100], Loss: 0.0782\n",
      "Epoch [20/100], Loss: 0.0788\n",
      "Epoch [21/100], Loss: 0.0785\n",
      "Epoch [22/100], Loss: 0.0776\n",
      "Epoch [23/100], Loss: 0.0796\n",
      "Epoch [24/100], Loss: 0.0765\n",
      "Epoch [25/100], Loss: 0.0813\n",
      "Epoch [26/100], Loss: 0.0754\n",
      "Epoch [27/100], Loss: 0.0845\n",
      "Epoch [28/100], Loss: 0.0751\n",
      "Epoch [29/100], Loss: 0.0886\n",
      "Epoch [30/100], Loss: 0.0780\n",
      "Epoch [31/100], Loss: 0.0879\n",
      "Epoch [32/100], Loss: 0.0823\n",
      "Epoch [33/100], Loss: 0.0813\n",
      "Epoch [34/100], Loss: 0.0814\n",
      "Epoch [35/100], Loss: 0.0785\n",
      "Epoch [36/100], Loss: 0.0794\n",
      "Epoch [37/100], Loss: 0.0784\n",
      "Epoch [38/100], Loss: 0.0782\n",
      "Epoch [39/100], Loss: 0.0789\n",
      "Epoch [40/100], Loss: 0.0773\n",
      "Epoch [41/100], Loss: 0.0797\n",
      "Epoch [42/100], Loss: 0.0767\n",
      "Epoch [43/100], Loss: 0.0808\n",
      "Epoch [44/100], Loss: 0.0762\n",
      "Epoch [45/100], Loss: 0.0826\n",
      "Epoch [46/100], Loss: 0.0760\n",
      "Epoch [47/100], Loss: 0.0846\n",
      "Epoch [48/100], Loss: 0.0770\n",
      "Epoch [49/100], Loss: 0.0851\n",
      "Epoch [50/100], Loss: 0.0793\n",
      "Epoch [51/100], Loss: 0.0825\n",
      "Epoch [52/100], Loss: 0.0801\n",
      "Epoch [53/100], Loss: 0.0800\n",
      "Epoch [54/100], Loss: 0.0794\n",
      "Epoch [55/100], Loss: 0.0789\n",
      "Epoch [56/100], Loss: 0.0786\n",
      "Epoch [57/100], Loss: 0.0788\n",
      "Epoch [58/100], Loss: 0.0779\n",
      "Epoch [59/100], Loss: 0.0791\n",
      "Epoch [60/100], Loss: 0.0774\n",
      "Epoch [61/100], Loss: 0.0795\n",
      "Epoch [62/100], Loss: 0.0770\n",
      "Epoch [63/100], Loss: 0.0802\n",
      "Epoch [64/100], Loss: 0.0766\n",
      "Epoch [65/100], Loss: 0.0811\n",
      "Epoch [66/100], Loss: 0.0765\n",
      "Epoch [67/100], Loss: 0.0822\n",
      "Epoch [68/100], Loss: 0.0767\n",
      "Epoch [69/100], Loss: 0.0832\n",
      "Epoch [70/100], Loss: 0.0777\n",
      "Epoch [71/100], Loss: 0.0828\n",
      "Epoch [72/100], Loss: 0.0787\n",
      "Epoch [73/100], Loss: 0.0809\n",
      "Epoch [74/100], Loss: 0.0793\n",
      "Epoch [75/100], Loss: 0.0791\n",
      "Epoch [76/100], Loss: 0.0789\n",
      "Epoch [77/100], Loss: 0.0785\n",
      "Epoch [78/100], Loss: 0.0783\n",
      "Epoch [79/100], Loss: 0.0784\n",
      "Epoch [80/100], Loss: 0.0778\n",
      "Epoch [81/100], Loss: 0.0786\n",
      "Epoch [82/100], Loss: 0.0775\n",
      "Epoch [83/100], Loss: 0.0788\n",
      "Epoch [84/100], Loss: 0.0772\n",
      "Epoch [85/100], Loss: 0.0794\n",
      "Epoch [86/100], Loss: 0.0768\n",
      "Epoch [87/100], Loss: 0.0800\n",
      "Epoch [88/100], Loss: 0.0766\n",
      "Epoch [89/100], Loss: 0.0808\n",
      "Epoch [90/100], Loss: 0.0766\n",
      "Epoch [91/100], Loss: 0.0815\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Epoch [93/100], Loss: 0.0819\n",
      "Epoch [94/100], Loss: 0.0776\n",
      "Epoch [95/100], Loss: 0.0815\n",
      "Epoch [96/100], Loss: 0.0784\n",
      "Epoch [97/100], Loss: 0.0801\n",
      "Epoch [98/100], Loss: 0.0786\n",
      "Epoch [99/100], Loss: 0.0799\n",
      "Epoch [100/100], Loss: 0.0775\n",
      "Validation Loss for fold 2: 0.0418\n",
      "Training fold 3/5\n",
      "Epoch [1/100], Loss: 0.2142\n",
      "Epoch [2/100], Loss: 0.0588\n",
      "Epoch [3/100], Loss: 0.0680\n",
      "Epoch [4/100], Loss: 0.1854\n",
      "Epoch [5/100], Loss: 0.0684\n",
      "Epoch [6/100], Loss: 0.0685\n",
      "Epoch [7/100], Loss: 0.1718\n",
      "Epoch [8/100], Loss: 0.0813\n",
      "Epoch [9/100], Loss: 0.1622\n",
      "Epoch [10/100], Loss: 0.0581\n",
      "Epoch [11/100], Loss: 0.1509\n",
      "Epoch [12/100], Loss: 0.0493\n",
      "Epoch [13/100], Loss: 0.1662\n",
      "Epoch [14/100], Loss: 0.0638\n",
      "Epoch [15/100], Loss: 0.1681\n",
      "Epoch [16/100], Loss: 0.1602\n",
      "Epoch [17/100], Loss: 0.1442\n",
      "Epoch [18/100], Loss: 0.1314\n",
      "Epoch [19/100], Loss: 0.1206\n",
      "Epoch [20/100], Loss: 0.1016\n",
      "Epoch [21/100], Loss: 0.0857\n",
      "Epoch [22/100], Loss: 0.0852\n",
      "Epoch [23/100], Loss: 0.0886\n",
      "Epoch [24/100], Loss: 0.0862\n",
      "Epoch [25/100], Loss: 0.0877\n",
      "Epoch [26/100], Loss: 0.0876\n",
      "Epoch [27/100], Loss: 0.0857\n",
      "Epoch [28/100], Loss: 0.0868\n",
      "Epoch [29/100], Loss: 0.0855\n",
      "Epoch [30/100], Loss: 0.0863\n",
      "Epoch [31/100], Loss: 0.0857\n",
      "Epoch [32/100], Loss: 0.0860\n",
      "Epoch [33/100], Loss: 0.0858\n",
      "Epoch [34/100], Loss: 0.0859\n",
      "Epoch [35/100], Loss: 0.0858\n",
      "Epoch [36/100], Loss: 0.0859\n",
      "Epoch [37/100], Loss: 0.0859\n",
      "Epoch [38/100], Loss: 0.0859\n",
      "Epoch [39/100], Loss: 0.0859\n",
      "Epoch [40/100], Loss: 0.0859\n",
      "Epoch [41/100], Loss: 0.0859\n",
      "Epoch [42/100], Loss: 0.0859\n",
      "Epoch [43/100], Loss: 0.0859\n",
      "Epoch [44/100], Loss: 0.0859\n",
      "Epoch [45/100], Loss: 0.0859\n",
      "Epoch [46/100], Loss: 0.0859\n",
      "Epoch [47/100], Loss: 0.0859\n",
      "Epoch [48/100], Loss: 0.0859\n",
      "Epoch [49/100], Loss: 0.0858\n",
      "Epoch [50/100], Loss: 0.0859\n",
      "Epoch [51/100], Loss: 0.0858\n",
      "Epoch [52/100], Loss: 0.0858\n",
      "Epoch [53/100], Loss: 0.0858\n",
      "Epoch [54/100], Loss: 0.0858\n",
      "Epoch [55/100], Loss: 0.0858\n",
      "Epoch [56/100], Loss: 0.0858\n",
      "Epoch [57/100], Loss: 0.0858\n",
      "Epoch [58/100], Loss: 0.0858\n",
      "Epoch [59/100], Loss: 0.0858\n",
      "Epoch [60/100], Loss: 0.0858\n",
      "Epoch [61/100], Loss: 0.0858\n",
      "Epoch [62/100], Loss: 0.0858\n",
      "Epoch [63/100], Loss: 0.0858\n",
      "Epoch [64/100], Loss: 0.0858\n",
      "Epoch [65/100], Loss: 0.0858\n",
      "Epoch [66/100], Loss: 0.0858\n",
      "Epoch [67/100], Loss: 0.0858\n",
      "Epoch [68/100], Loss: 0.0858\n",
      "Epoch [69/100], Loss: 0.0858\n",
      "Epoch [70/100], Loss: 0.0858\n",
      "Epoch [71/100], Loss: 0.0858\n",
      "Epoch [72/100], Loss: 0.0858\n",
      "Epoch [73/100], Loss: 0.0858\n",
      "Epoch [74/100], Loss: 0.0858\n",
      "Epoch [75/100], Loss: 0.0857\n",
      "Epoch [76/100], Loss: 0.0858\n",
      "Epoch [77/100], Loss: 0.0857\n",
      "Epoch [78/100], Loss: 0.0857\n",
      "Epoch [79/100], Loss: 0.0857\n",
      "Epoch [80/100], Loss: 0.0857\n",
      "Epoch [81/100], Loss: 0.0857\n",
      "Epoch [82/100], Loss: 0.0857\n",
      "Epoch [83/100], Loss: 0.0857\n",
      "Epoch [84/100], Loss: 0.0857\n",
      "Epoch [85/100], Loss: 0.0857\n",
      "Epoch [86/100], Loss: 0.0857\n",
      "Epoch [87/100], Loss: 0.0857\n",
      "Epoch [88/100], Loss: 0.0857\n",
      "Epoch [89/100], Loss: 0.0857\n",
      "Epoch [90/100], Loss: 0.0857\n",
      "Epoch [91/100], Loss: 0.0857\n",
      "Epoch [92/100], Loss: 0.0857\n",
      "Epoch [93/100], Loss: 0.0857\n",
      "Epoch [94/100], Loss: 0.0856\n",
      "Epoch [95/100], Loss: 0.0856\n",
      "Epoch [96/100], Loss: 0.0856\n",
      "Epoch [97/100], Loss: 0.0856\n",
      "Epoch [98/100], Loss: 0.0856\n",
      "Epoch [99/100], Loss: 0.0856\n",
      "Epoch [100/100], Loss: 0.0856\n",
      "Validation Loss for fold 3: 0.0439\n",
      "Training fold 4/5\n",
      "Epoch [1/100], Loss: 0.2605\n",
      "Epoch [2/100], Loss: 0.0511\n",
      "Epoch [3/100], Loss: 0.0500\n",
      "Epoch [4/100], Loss: 0.2213\n",
      "Epoch [5/100], Loss: 0.1835\n",
      "Epoch [6/100], Loss: 0.0584\n",
      "Epoch [7/100], Loss: 0.2275\n",
      "Epoch [8/100], Loss: 0.2094\n",
      "Epoch [9/100], Loss: 0.0514\n",
      "Epoch [10/100], Loss: 0.1815\n",
      "Epoch [11/100], Loss: 0.0520\n",
      "Epoch [12/100], Loss: 0.1847\n",
      "Epoch [13/100], Loss: 0.1887\n",
      "Epoch [14/100], Loss: 0.1458\n",
      "Epoch [15/100], Loss: 0.0508\n",
      "Epoch [16/100], Loss: 0.1276\n",
      "Epoch [17/100], Loss: 0.1308\n",
      "Epoch [18/100], Loss: 0.0581\n",
      "Epoch [19/100], Loss: 0.1073\n",
      "Epoch [20/100], Loss: 0.1125\n",
      "Epoch [21/100], Loss: 0.0692\n",
      "Epoch [22/100], Loss: 0.0980\n",
      "Epoch [23/100], Loss: 0.0773\n",
      "Epoch [24/100], Loss: 0.0704\n",
      "Epoch [25/100], Loss: 0.0789\n",
      "Epoch [26/100], Loss: 0.0734\n",
      "Epoch [27/100], Loss: 0.0737\n",
      "Epoch [28/100], Loss: 0.0751\n",
      "Epoch [29/100], Loss: 0.0741\n",
      "Epoch [30/100], Loss: 0.0743\n",
      "Epoch [31/100], Loss: 0.0745\n",
      "Epoch [32/100], Loss: 0.0743\n",
      "Epoch [33/100], Loss: 0.0744\n",
      "Epoch [34/100], Loss: 0.0744\n",
      "Epoch [35/100], Loss: 0.0743\n",
      "Epoch [36/100], Loss: 0.0744\n",
      "Epoch [37/100], Loss: 0.0743\n",
      "Epoch [38/100], Loss: 0.0743\n",
      "Epoch [39/100], Loss: 0.0743\n",
      "Epoch [40/100], Loss: 0.0743\n",
      "Epoch [41/100], Loss: 0.0744\n",
      "Epoch [42/100], Loss: 0.0743\n",
      "Epoch [43/100], Loss: 0.0744\n",
      "Epoch [44/100], Loss: 0.0743\n",
      "Epoch [45/100], Loss: 0.0743\n",
      "Epoch [46/100], Loss: 0.0744\n",
      "Epoch [47/100], Loss: 0.0743\n",
      "Epoch [48/100], Loss: 0.0744\n",
      "Epoch [49/100], Loss: 0.0744\n",
      "Epoch [50/100], Loss: 0.0744\n",
      "Epoch [51/100], Loss: 0.0744\n",
      "Epoch [52/100], Loss: 0.0744\n",
      "Epoch [53/100], Loss: 0.0744\n",
      "Epoch [54/100], Loss: 0.0744\n",
      "Epoch [55/100], Loss: 0.0744\n",
      "Epoch [56/100], Loss: 0.0744\n",
      "Epoch [57/100], Loss: 0.0744\n",
      "Epoch [58/100], Loss: 0.0744\n",
      "Epoch [59/100], Loss: 0.0744\n",
      "Epoch [60/100], Loss: 0.0744\n",
      "Epoch [61/100], Loss: 0.0744\n",
      "Epoch [62/100], Loss: 0.0744\n",
      "Epoch [63/100], Loss: 0.0744\n",
      "Epoch [64/100], Loss: 0.0744\n",
      "Epoch [65/100], Loss: 0.0744\n",
      "Epoch [66/100], Loss: 0.0744\n",
      "Epoch [67/100], Loss: 0.0744\n",
      "Epoch [68/100], Loss: 0.0744\n",
      "Epoch [69/100], Loss: 0.0744\n",
      "Epoch [70/100], Loss: 0.0744\n",
      "Epoch [71/100], Loss: 0.0744\n",
      "Epoch [72/100], Loss: 0.0743\n",
      "Epoch [73/100], Loss: 0.0744\n",
      "Epoch [74/100], Loss: 0.0743\n",
      "Epoch [75/100], Loss: 0.0744\n",
      "Epoch [76/100], Loss: 0.0743\n",
      "Epoch [77/100], Loss: 0.0744\n",
      "Epoch [78/100], Loss: 0.0743\n",
      "Epoch [79/100], Loss: 0.0744\n",
      "Epoch [80/100], Loss: 0.0743\n",
      "Epoch [81/100], Loss: 0.0744\n",
      "Epoch [82/100], Loss: 0.0743\n",
      "Epoch [83/100], Loss: 0.0745\n",
      "Epoch [84/100], Loss: 0.0741\n",
      "Epoch [85/100], Loss: 0.0747\n",
      "Epoch [86/100], Loss: 0.0739\n",
      "Epoch [87/100], Loss: 0.0751\n",
      "Epoch [88/100], Loss: 0.0733\n",
      "Epoch [89/100], Loss: 0.0763\n",
      "Epoch [90/100], Loss: 0.0721\n",
      "Epoch [91/100], Loss: 0.0793\n",
      "Epoch [92/100], Loss: 0.0712\n",
      "Epoch [93/100], Loss: 0.0843\n",
      "Epoch [94/100], Loss: 0.0765\n",
      "Epoch [95/100], Loss: 0.0787\n",
      "Epoch [96/100], Loss: 0.0792\n",
      "Epoch [97/100], Loss: 0.0735\n",
      "Epoch [98/100], Loss: 0.0766\n",
      "Epoch [99/100], Loss: 0.0731\n",
      "Epoch [100/100], Loss: 0.0753\n",
      "Validation Loss for fold 4: 0.0345\n",
      "Training fold 5/5\n",
      "Epoch [1/100], Loss: 0.1232\n",
      "Epoch [2/100], Loss: 0.1888\n",
      "Epoch [3/100], Loss: 0.1325\n",
      "Epoch [4/100], Loss: 0.0826\n",
      "Epoch [5/100], Loss: 0.2306\n",
      "Epoch [6/100], Loss: 0.2149\n",
      "Epoch [7/100], Loss: 0.0426\n",
      "Epoch [8/100], Loss: 0.1816\n",
      "Epoch [9/100], Loss: 0.0421\n",
      "Epoch [10/100], Loss: 0.1916\n",
      "Epoch [11/100], Loss: 0.2004\n",
      "Epoch [12/100], Loss: 0.1529\n",
      "Epoch [13/100], Loss: 0.0486\n",
      "Epoch [14/100], Loss: 0.1184\n",
      "Epoch [15/100], Loss: 0.1368\n",
      "Epoch [16/100], Loss: 0.0790\n",
      "Epoch [17/100], Loss: 0.0548\n",
      "Epoch [18/100], Loss: 0.0660\n",
      "Epoch [19/100], Loss: 0.0645\n",
      "Epoch [20/100], Loss: 0.0610\n",
      "Epoch [21/100], Loss: 0.0640\n",
      "Epoch [22/100], Loss: 0.0624\n",
      "Epoch [23/100], Loss: 0.0629\n",
      "Epoch [24/100], Loss: 0.0630\n",
      "Epoch [25/100], Loss: 0.0627\n",
      "Epoch [26/100], Loss: 0.0629\n",
      "Epoch [27/100], Loss: 0.0627\n",
      "Epoch [28/100], Loss: 0.0629\n",
      "Epoch [29/100], Loss: 0.0628\n",
      "Epoch [30/100], Loss: 0.0628\n",
      "Epoch [31/100], Loss: 0.0628\n",
      "Epoch [32/100], Loss: 0.0628\n",
      "Epoch [33/100], Loss: 0.0628\n",
      "Epoch [34/100], Loss: 0.0628\n",
      "Epoch [35/100], Loss: 0.0628\n",
      "Epoch [36/100], Loss: 0.0628\n",
      "Epoch [37/100], Loss: 0.0628\n",
      "Epoch [38/100], Loss: 0.0629\n",
      "Epoch [39/100], Loss: 0.0628\n",
      "Epoch [40/100], Loss: 0.0629\n",
      "Epoch [41/100], Loss: 0.0628\n",
      "Epoch [42/100], Loss: 0.0629\n",
      "Epoch [43/100], Loss: 0.0627\n",
      "Epoch [44/100], Loss: 0.0631\n",
      "Epoch [45/100], Loss: 0.0624\n",
      "Epoch [46/100], Loss: 0.0635\n",
      "Epoch [47/100], Loss: 0.0618\n",
      "Epoch [48/100], Loss: 0.0649\n",
      "Epoch [49/100], Loss: 0.0601\n",
      "Epoch [50/100], Loss: 0.0693\n",
      "Epoch [51/100], Loss: 0.0574\n",
      "Epoch [52/100], Loss: 0.0822\n",
      "Epoch [53/100], Loss: 0.0677\n",
      "Epoch [54/100], Loss: 0.0731\n",
      "Epoch [55/100], Loss: 0.0730\n",
      "Epoch [56/100], Loss: 0.0604\n",
      "Epoch [57/100], Loss: 0.0660\n",
      "Epoch [58/100], Loss: 0.0616\n",
      "Epoch [59/100], Loss: 0.0637\n",
      "Epoch [60/100], Loss: 0.0625\n",
      "Epoch [61/100], Loss: 0.0630\n",
      "Epoch [62/100], Loss: 0.0629\n",
      "Epoch [63/100], Loss: 0.0626\n",
      "Epoch [64/100], Loss: 0.0632\n",
      "Epoch [65/100], Loss: 0.0624\n",
      "Epoch [66/100], Loss: 0.0635\n",
      "Epoch [67/100], Loss: 0.0621\n",
      "Epoch [68/100], Loss: 0.0641\n",
      "Epoch [69/100], Loss: 0.0616\n",
      "Epoch [70/100], Loss: 0.0653\n",
      "Epoch [71/100], Loss: 0.0609\n",
      "Epoch [72/100], Loss: 0.0678\n",
      "Epoch [73/100], Loss: 0.0607\n",
      "Epoch [74/100], Loss: 0.0711\n",
      "Epoch [75/100], Loss: 0.0638\n",
      "Epoch [76/100], Loss: 0.0693\n",
      "Epoch [77/100], Loss: 0.0669\n",
      "Epoch [78/100], Loss: 0.0638\n",
      "Epoch [79/100], Loss: 0.0654\n",
      "Epoch [80/100], Loss: 0.0626\n",
      "Epoch [81/100], Loss: 0.0640\n",
      "Epoch [82/100], Loss: 0.0627\n",
      "Epoch [83/100], Loss: 0.0632\n",
      "Epoch [84/100], Loss: 0.0630\n",
      "Epoch [85/100], Loss: 0.0628\n",
      "Epoch [86/100], Loss: 0.0633\n",
      "Epoch [87/100], Loss: 0.0625\n",
      "Epoch [88/100], Loss: 0.0635\n",
      "Epoch [89/100], Loss: 0.0623\n",
      "Epoch [90/100], Loss: 0.0639\n",
      "Epoch [91/100], Loss: 0.0621\n",
      "Epoch [92/100], Loss: 0.0645\n",
      "Epoch [93/100], Loss: 0.0618\n",
      "Epoch [94/100], Loss: 0.0655\n",
      "Epoch [95/100], Loss: 0.0616\n",
      "Epoch [96/100], Loss: 0.0668\n",
      "Epoch [97/100], Loss: 0.0622\n",
      "Epoch [98/100], Loss: 0.0674\n",
      "Epoch [99/100], Loss: 0.0636\n",
      "Epoch [100/100], Loss: 0.0660\n",
      "Validation Loss for fold 5: 0.0456\n",
      "Average Validation Loss across 5 folds: 0.0431\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fold_accuracies = []\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(X)):\n",
    "    print(f\"Training fold {fold + 1}/{k}\")\n",
    "    \n",
    "    # Split the data into training and validation sets\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = AminoAcidCNN(SEQ_LENGTH, len(amino_acids))\n",
    "    \n",
    "    # Define the optimizer and loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
    "    criterion = nn.MSELoss()  # Mean Squared Error for regression\n",
    "\n",
    "    # Training the model\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Create batches for training\n",
    "        for i in range(0, len(X_train), BATCH_SIZE):\n",
    "            X_batch = X_train[i:i + BATCH_SIZE]\n",
    "            y_batch = y_train[i:i + BATCH_SIZE]\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch.view(-1, 1))\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Print loss for each epoch\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}], Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_val)\n",
    "        val_loss = criterion(val_outputs, y_val.view(-1, 1))\n",
    "        fold_accuracies.append(val_loss.item())\n",
    "        print(f\"Validation Loss for fold {fold + 1}: {val_loss.item():.4f}\")\n",
    "\n",
    "# Average validation loss across all folds\n",
    "avg_loss = np.mean(fold_accuracies)\n",
    "print(f\"Average Validation Loss across {k} folds: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30486666780170124"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "def compute_predictions_and_spearman(df, model):\n",
    "    # Get the data from the dataframe\n",
    "    X, y = get_data(df)\n",
    "\n",
    "    # Ensure the model is in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Compute predictions\n",
    "    with torch.no_grad():\n",
    "        predicted_dms_scores = model(X).squeeze().numpy()  # Flatten to 1D array\n",
    "\n",
    "    # Compute Spearman correlation\n",
    "    correlation, _ = spearmanr(predicted_dms_scores, y.numpy())  # Compute Spearman correlation between actual and predicted DMS scores\n",
    "\n",
    "    return predicted_dms_scores, correlation\n",
    "\n",
    "_, correlation = compute_predictions_and_spearman(df, model)\n",
    "correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compbio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
