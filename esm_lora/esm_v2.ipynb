{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df = pd.read_csv('../train.csv')\n",
    "\n",
    "# get the sequence\n",
    "seq = open('../sequence.fasta', 'r').read()\n",
    "seq = seq.split(\"\\n\")[1]\n",
    "\n",
    "# create each mutated sequence using the info\n",
    "sequences = []\n",
    "for i in train_data_df['mutant']:\n",
    "    ind = int(i[1:-1])\n",
    "    tmp = seq[:ind] + i[-1] + seq[ind+1:]\n",
    "    sequences.append(tmp)\n",
    "train_data_df['Sequence'] = sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mutant</th>\n",
       "      <th>labels</th>\n",
       "      <th>Sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M0Y</td>\n",
       "      <td>0.2730</td>\n",
       "      <td>YVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M0W</td>\n",
       "      <td>0.2857</td>\n",
       "      <td>WVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M0V</td>\n",
       "      <td>0.2153</td>\n",
       "      <td>VVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M0T</td>\n",
       "      <td>0.3122</td>\n",
       "      <td>TVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M0S</td>\n",
       "      <td>0.2180</td>\n",
       "      <td>SVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  mutant  labels                                           Sequence\n",
       "0    M0Y  0.2730  YVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...\n",
       "1    M0W  0.2857  WVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...\n",
       "2    M0V  0.2153  VVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...\n",
       "3    M0T  0.3122  TVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...\n",
       "4    M0S  0.2180  SVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df = train_data_df.rename(columns={'DMS_score': 'labels'})\n",
    "\n",
    "train_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(train_data_df[\"labels\"])\n",
    "plt.xlabel(\"DMS Score\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "def compute_metrics(p):\n",
    "    \"\"\"Compute Pearson correlation for evaluation.\"\"\"\n",
    "    predictions, labels = p\n",
    "    \n",
    "    # Flatten in case of multi-dimensional outputs\n",
    "    predictions = predictions.flatten()\n",
    "    labels = labels.flatten()\n",
    "    \n",
    "    # Compute Pearson correlation\n",
    "    pearson_corr, _ = pearsonr(predictions, labels)\n",
    "    \n",
    "    return {'pearson_correlation': pearson_corr}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, inputs):\n",
    "    \"\"\"Custom compute_loss function for regression.\"\"\"\n",
    "    logits = model(**inputs).logits.view(-1)  # Flatten logits\n",
    "    labels = inputs[\"labels\"].view(-1)  # Flatten labels\n",
    "    loss_fct = nn.MSELoss()  # Mean Squared Error Loss\n",
    "    loss = loss_fct(logits, labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_data = Dataset.from_pandas(train_data_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = train_data.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "test_dataset = train_test_split[\"test\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Assuming you have the DataFrame `train_data_df`\n",
    "# Tokenization\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t12_35M_UR50D\")\n",
    "\n",
    "# Max sequence length (for example, you can set it based on your sequences' length)\n",
    "max_sequence_length = 656  # Adjust this based on your data or preference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the sequences\n",
    "train_data_tokenized = tokenizer(\n",
    "    train_dataset[\"Sequence\"],\n",
    "    max_length=max_sequence_length,\n",
    "    return_tensors=\"pt\",  # Returns PyTorch tensors\n",
    "    padding=True,         # Pads the sequences to max length\n",
    "    truncation=True       # Truncates if the length exceeds max_sequence_length\n",
    ")\n",
    "test_data_tokenized = tokenizer(\n",
    "    test_dataset[\"Sequence\"],\n",
    "    max_length=max_sequence_length,\n",
    "    return_tensors=\"pt\",  # Returns PyTorch tensors\n",
    "    padding=True,         # Pads the sequences to max length\n",
    "    truncation=True       # Truncates if the length exceeds max_sequence_length\n",
    ")\n",
    "\n",
    "train_labels = train_dataset[\"labels\"]\n",
    "test_labels = test_dataset[\"labels\"]\n",
    "\n",
    "train_ds = Dataset.from_dict({k: v for k, v in train_data_tokenized.items()}).add_column(\"labels\", train_labels)\n",
    "test_ds = Dataset.from_dict({k: v for k, v in test_data_tokenized.items()}).add_column(\"labels\", test_labels)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_function_no_sweeps(train_dataset, test_dataset):\n",
    "    \n",
    "    # Set the LoRA config\n",
    "    config = {\n",
    "        \"lora_alpha\": 1, #try 0.5, 1, 2, ..., 16\n",
    "        \"lora_dropout\": 0.2,\n",
    "        \"lr\": 5.701568055793089e-04,\n",
    "        \"lr_scheduler_type\": \"cosine\",\n",
    "        \"max_grad_norm\": 0.5,\n",
    "        \"num_train_epochs\": 3,\n",
    "        \"per_device_train_batch_size\": 12,\n",
    "        \"r\": 8,\n",
    "        \"weight_decay\": 0.2,\n",
    "        # Add other hyperparameters as needed\n",
    "    }\n",
    "    # The base model you will train a LoRA on top of\n",
    "    model_checkpoint = \"facebook/esm2_t12_35M_UR50D\"  \n",
    "    \n",
    "    # Define labels and model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=1)\n",
    "\n",
    "    # Convert the model into a PeftModel\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_CLS, \n",
    "        inference_mode=False, \n",
    "        r=config[\"r\"], \n",
    "        lora_alpha=config[\"lora_alpha\"], \n",
    "        target_modules=[\"query\", \"key\", \"value\"], # also try \"dense_h_to_4h\" and \"dense_4h_to_h\"\n",
    "        lora_dropout=config[\"lora_dropout\"], \n",
    "        bias=\"none\" # or \"all\" or \"lora_only\" \n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "    # Training setup\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"esm2_t12_35M-lora-binding-sites_{timestamp}\",\n",
    "        learning_rate=config[\"lr\"],\n",
    "        lr_scheduler_type=config[\"lr_scheduler_type\"],\n",
    "        gradient_accumulation_steps=1,\n",
    "        max_grad_norm=config[\"max_grad_norm\"],\n",
    "        per_device_train_batch_size=config[\"per_device_train_batch_size\"],\n",
    "        per_device_eval_batch_size=config[\"per_device_train_batch_size\"],\n",
    "        num_train_epochs=config[\"num_train_epochs\"],\n",
    "        weight_decay=config[\"weight_decay\"],\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"pearson_correlation\",\n",
    "        greater_is_better=True,\n",
    "        push_to_hub=False,\n",
    "        logging_dir=None,\n",
    "        logging_first_step=False,\n",
    "        logging_steps=200,\n",
    "        save_total_limit=7,\n",
    "        no_cuda=False,\n",
    "        seed=8893,\n",
    "    )\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    # Train and Save Model\n",
    "    trainer.train()\n",
    "    save_path = os.path.join(\"lora_binding_sites\", f\"best_model_esm2_t12_35M_lora_{timestamp}\")\n",
    "    trainer.save_model(save_path)\n",
    "    tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/hice1/rmehta98/.conda/envs/compbio/lib/python3.13/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_2824154/3484572140.py:62: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='228' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  3/228 00:00 < 01:10, 3.19 it/s, Epoch 0.03/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_function_no_sweeps(train_ds, test_ds)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "from datetime import datetime\n",
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from peft import PeftModel, LoraConfig, TaskType\n",
    "from sklearn.metrics import accuracy_score, matthews_corrcoef\n",
    "import torch\n",
    "\n",
    "# Define the grid of hyperparameters to search through\n",
    "lr_grid = [1e-4, 5e-4, 1e-3]  # Learning rates\n",
    "lora_alpha_grid = [0.5, 1, 2, 4, 8, 16]  # lora_alpha values\n",
    "r_grid = [4, 8, 16]  # r values\n",
    "weight_decay_grid = [0.1, 0.2, 0.3]  # Weight decay values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/hice1/rmehta98/.conda/envs/compbio/lib/python3.13/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='228' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  3/228 00:00 < 01:11, 3.15 it/s, Epoch 0.03/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2/19 00:00 < 00:02, 7.32 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.040679238736629486, 'eval_pearson_correlation': 0.02186392992734909, 'eval_runtime': 2.8973, 'eval_samples_per_second': 78.694, 'eval_steps_per_second': 6.558, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/hice1/rmehta98/.conda/envs/compbio/lib/python3.13/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='228' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  3/228 00:00 < 01:11, 3.17 it/s, Epoch 0.03/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2/19 00:00 < 00:02, 7.36 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.04067691043019295, 'eval_pearson_correlation': -0.037978529930114746, 'eval_runtime': 3.1294, 'eval_samples_per_second': 72.857, 'eval_steps_per_second': 6.071, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/rmehta98/.conda/envs/compbio/lib/python3.13/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='228' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  3/228 00:00 < 01:11, 3.15 it/s, Epoch 0.03/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2/19 00:00 < 00:02, 7.33 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.04067691043019295, 'eval_pearson_correlation': -0.038028351962566376, 'eval_runtime': 2.6609, 'eval_samples_per_second': 85.686, 'eval_steps_per_second': 7.14, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/hice1/rmehta98/.conda/envs/compbio/lib/python3.13/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='228' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  3/228 00:00 < 01:11, 3.14 it/s, Epoch 0.03/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2/19 00:00 < 00:02, 7.41 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.04067640379071236, 'eval_pearson_correlation': -0.03632612153887749, 'eval_runtime': 2.9151, 'eval_samples_per_second': 78.214, 'eval_steps_per_second': 6.518, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/hice1/rmehta98/.conda/envs/compbio/lib/python3.13/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='228' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  3/228 00:00 < 01:11, 3.15 it/s, Epoch 0.03/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2/19 00:00 < 00:02, 6.43 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.04067603498697281, 'eval_pearson_correlation': -0.036020562052726746, 'eval_runtime': 2.835, 'eval_samples_per_second': 80.424, 'eval_steps_per_second': 6.702, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/hice1/rmehta98/.conda/envs/compbio/lib/python3.13/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='228' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  3/228 00:00 < 01:11, 3.13 it/s, Epoch 0.03/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.04067632555961609, 'eval_pearson_correlation': -0.03617993742227554, 'eval_runtime': 2.7299, 'eval_samples_per_second': 83.519, 'eval_steps_per_second': 6.96, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/rmehta98/.conda/envs/compbio/lib/python3.13/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='228' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  3/228 00:00 < 01:13, 3.06 it/s, Epoch 0.03/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.040676116943359375, 'eval_pearson_correlation': -0.03620517626404762, 'eval_runtime': 2.6685, 'eval_samples_per_second': 85.442, 'eval_steps_per_second': 7.12, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/rmehta98/.conda/envs/compbio/lib/python3.13/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='228' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  5/228 00:01 < 01:15, 2.94 it/s, Epoch 0.05/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.040676262229681015, 'eval_pearson_correlation': -0.03630216792225838, 'eval_runtime': 2.9321, 'eval_samples_per_second': 77.761, 'eval_steps_per_second': 6.48, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/hice1/rmehta98/.conda/envs/compbio/lib/python3.13/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='228' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  3/228 00:00 < 01:12, 3.09 it/s, Epoch 0.03/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.040676411241292953, 'eval_pearson_correlation': -0.03638749569654465, 'eval_runtime': 2.9381, 'eval_samples_per_second': 77.602, 'eval_steps_per_second': 6.467, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/rmehta98/.conda/envs/compbio/lib/python3.13/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='228' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  3/228 00:00 < 01:11, 3.14 it/s, Epoch 0.03/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.04067512974143028, 'eval_pearson_correlation': -0.03290513530373573, 'eval_runtime': 2.6475, 'eval_samples_per_second': 86.12, 'eval_steps_per_second': 7.177, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/rmehta98/.conda/envs/compbio/lib/python3.13/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='228' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  3/228 00:00 < 01:12, 3.11 it/s, Epoch 0.03/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2/19 00:00 < 00:02, 7.37 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.040674954652786255, 'eval_pearson_correlation': -0.032617248594760895, 'eval_runtime': 2.7274, 'eval_samples_per_second': 83.595, 'eval_steps_per_second': 6.966, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/hice1/rmehta98/.conda/envs/compbio/lib/python3.13/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='228' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  3/228 00:00 < 01:12, 3.10 it/s, Epoch 0.03/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2/19 00:00 < 00:02, 7.35 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.04067522659897804, 'eval_pearson_correlation': -0.03297354280948639, 'eval_runtime': 2.6452, 'eval_samples_per_second': 86.195, 'eval_steps_per_second': 7.183, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/hice1/rmehta98/.conda/envs/compbio/lib/python3.13/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='228' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  3/228 00:00 < 01:11, 3.13 it/s, Epoch 0.03/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2/19 00:00 < 00:02, 7.39 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.04067537188529968, 'eval_pearson_correlation': -0.03306811302900314, 'eval_runtime': 2.702, 'eval_samples_per_second': 84.383, 'eval_steps_per_second': 7.032, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/hice1/rmehta98/.conda/envs/compbio/lib/python3.13/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='228' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  3/228 00:00 < 01:11, 3.15 it/s, Epoch 0.03/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Grid search for hyperparameters\n",
    "def grid_search(train_dataset, test_dataset):\n",
    "    # Define paths and model configuration\n",
    "    model_checkpoint = \"facebook/esm2_t12_35M_UR50D\"\n",
    "    \n",
    "    # Prepare grid for hyperparameter search\n",
    "    param_grid = {\n",
    "        \"lr\": lr_grid,\n",
    "        \"lora_alpha\": lora_alpha_grid,\n",
    "        \"r\": r_grid,\n",
    "        \"weight_decay\": weight_decay_grid,\n",
    "    }\n",
    "\n",
    "    # Generate all combinations of hyperparameters\n",
    "    param_combinations = list(itertools.product(*param_grid.values()))\n",
    "    \n",
    "    best_metrics = None\n",
    "    best_params = None\n",
    "\n",
    "    # Loop over all combinations of hyperparameters\n",
    "    for params in param_combinations:\n",
    "        config = {\n",
    "            \"lr\": params[0],\n",
    "            \"lora_alpha\": params[1],\n",
    "            \"r\": params[2],\n",
    "            \"weight_decay\": params[3],\n",
    "            \"lora_dropout\": 0.2,\n",
    "            \"num_train_epochs\": 3,\n",
    "            \"per_device_train_batch_size\": 12,\n",
    "            \"max_grad_norm\": 0.5,\n",
    "        }\n",
    "\n",
    "        # Load the base model and modify for LoRA\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=1)\n",
    "        \n",
    "        # LoRA config\n",
    "        peft_config = LoraConfig(\n",
    "            task_type=TaskType.SEQ_CLS, \n",
    "            inference_mode=False, \n",
    "            r=config[\"r\"], \n",
    "            lora_alpha=config[\"lora_alpha\"], \n",
    "            target_modules=[\"query\", \"key\", \"value\"],\n",
    "            lora_dropout=config[\"lora_dropout\"], \n",
    "            bias=\"none\" \n",
    "        )\n",
    "        \n",
    "        model = get_peft_model(model, peft_config)\n",
    "\n",
    "        # Create the output path with timestamp\n",
    "        timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "        # Setup the Trainer's training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"esm2_t12_35M-lora-binding-sites_{timestamp}\",\n",
    "            learning_rate=config[\"lr\"],\n",
    "            weight_decay=config[\"weight_decay\"],\n",
    "            max_grad_norm=config[\"max_grad_norm\"],\n",
    "            per_device_train_batch_size=config[\"per_device_train_batch_size\"],\n",
    "            per_device_eval_batch_size=config[\"per_device_train_batch_size\"],\n",
    "            num_train_epochs=config[\"num_train_epochs\"],\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"pearson_correlation\",\n",
    "            greater_is_better=True,\n",
    "            logging_dir=None,\n",
    "            logging_steps=200,\n",
    "            save_total_limit=7,\n",
    "            no_cuda=False,\n",
    "            seed=8893,\n",
    "        )\n",
    "\n",
    "        # Initialize Trainer\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=test_dataset,\n",
    "            compute_metrics=compute_metrics\n",
    "        )\n",
    "\n",
    "        # Train and evaluate the model\n",
    "        trainer.train()\n",
    "\n",
    "        # Evaluate the model on the test dataset\n",
    "        eval_results = trainer.evaluate()\n",
    "        print(eval_results)\n",
    "\n",
    "        # Store the best model and metrics\n",
    "        if best_metrics is None or eval_results[\"eval_pearson_correlation\"] > best_metrics[\"eval_pearson_correlation\"]:\n",
    "            best_metrics = eval_results\n",
    "            best_params = config\n",
    "\n",
    "        # Save the model after training\n",
    "        save_path = os.path.join(\"lora_binding_sites\", f\"best_model_esm2_t12_35M_lora_{timestamp}\")\n",
    "        trainer.save_model(save_path)\n",
    "        \n",
    "    print(f\"Best Hyperparameters: {best_params}\")\n",
    "    print(f\"Best Metrics: {best_metrics}\")\n",
    "\n",
    "# Run the grid search\n",
    "grid_search(train_ds, test_ds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "def compute_metrics_eval(model, ds):\n",
    "    \"\"\"Compute Pearson correlation for evaluation.\"\"\"\n",
    "    # Get the predictions using the trained model\n",
    "    trainer = Trainer(model=model)\n",
    "    predictions, labels, _ = trainer.predict(test_dataset=ds)\n",
    "    \n",
    "    # Remove padding and special tokens\n",
    "    print(predictions.shape)\n",
    "    print(labels.shape)\n",
    "\n",
    "    \n",
    "    # Flatten in case of multi-dimensional outputs\n",
    "    predictions = predictions.flatten()\n",
    "    labels = labels.flatten()\n",
    "    \n",
    "    # Compute Pearson correlation\n",
    "    pearson_corr, _ = pearsonr(predictions, labels)\n",
    "    \n",
    "    return {'pearson_correlation': pearson_corr}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='114' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  2/114 00:00 < 00:10, 10.82 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(912, 1)\n",
      "(912,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='29' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2/29 00:00 < 00:02, 10.91 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(228, 1)\n",
      "(228,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'pearson_correlation': 0.13680005}, {'pearson_correlation': 0.043371588})"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import(\n",
    "    matthews_corrcoef, \n",
    "    accuracy_score, \n",
    "    precision_recall_fscore_support, \n",
    "    roc_auc_score\n",
    ")\n",
    "from peft import PeftModel\n",
    "\n",
    "# Define paths to the LoRA and base models\n",
    "base_model_path = \"facebook/esm2_t12_35M_UR50D\"\n",
    "lora_model_path = \"./lora_binding_sites/best_model_esm2_t12_35M_lora_2025-03-30_21-03-25/\" # \"path/to/your/lora/model\" Replace with the correct path to your LoRA model\n",
    "\n",
    "# Load the base model\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(base_model_path, num_labels=1)\n",
    "\n",
    "# Load the LoRA model\n",
    "model = PeftModel.from_pretrained(base_model, lora_model_path)\n",
    "\n",
    "# Get the metrics for the training and test datasets\n",
    "train_metrics = compute_metrics_eval(model, train_ds)\n",
    "test_metrics = compute_metrics_eval(model, test_ds)\n",
    "\n",
    "train_metrics, test_metrics\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Set Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2361]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# Path to the saved LoRA model\n",
    "model_path = \"./lora_binding_sites/best_model_esm2_t12_35M_lora_2025-03-30_20-50-31/\"\n",
    "# ESM2 base model\n",
    "base_model_path = \"facebook/esm2_t12_35M_UR50D\"\n",
    "\n",
    "# Load the model\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(base_model_path, num_labels=1)\n",
    "loaded_model = PeftModel.from_pretrained(base_model, model_path)\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "loaded_model.eval()\n",
    "\n",
    "# Load the tokenizer\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "\n",
    "# Protein sequence for inference\n",
    "protein_sequence = \"MVNLARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLREKMRRRLESGDKWFSLEFFPPRTAEGAVNLISRFDRMAAGGPLYIDVTWHPAGDPGSDKETSSMMIASTAVNYCGLETILHMTCCRQRLEEITGHLHKAKQLGLKNIMALRGDPIGDQWEEEEGGFNYAVDLVKHIRSEFGDYFDICVAGYPKGHPEAGSFEADLKHLKEKVSAGADFIITQLFFEADTFFRFVKACTDMGITCPIVPGIFPIQGYHSLRQLVKLSKLEVPQEIKDVIEPIKDNDAAIRNYGIELAVSLCQELLASGLVPGLHFYTLNREMATTEVLKRLGMWTEDPRRPLPWALSAHPKRREEDVRPIFWASRPKSYIYRTQEWDEFPNGRWGNSSSPAFGELKDYYLFYLKSKSPKEELLKMWGEELTSEESVFEVFVLYLSGEPNRNGHKVTCLPWNDEPLAAETSLLKEELLRVNRQGILTINSQPNINGKPSSDPIVGWGPSGGYVFQKAYLEFFTSRETAEALLQVLKKYELRVNYHLVNVKGENITNAPELQPNAVTWGIFPGREIIQPTVVDPVSFMFWKDEAFALWIERWGKLYEEESPSRTIIQYIHDNYFLVNLVDNDFPLDNCLWQVVEDTLELLNRPTQNARETEAP\"  # Replace with your actual sequence\n",
    "\n",
    "# Tokenize the sequence\n",
    "inputs = loaded_tokenizer(protein_sequence, return_tensors=\"pt\", truncation=True, max_length=656, padding='max_length')\n",
    "\n",
    "# Run the model\n",
    "with torch.no_grad():\n",
    "    logits = loaded_model(**inputs).logits\n",
    "\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compbio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
