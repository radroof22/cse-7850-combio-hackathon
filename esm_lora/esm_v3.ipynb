{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/rmehta98/.conda/envs/compbio/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import pearsonr\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    PeftModel # Added for loading later\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "MODEL_NAME = \"facebook/esm2_t12_35M_UR50D\"  # Or choose a larger ESM model\n",
    "DATA_FILE = \"../protein_embeddings.parquet\"      # <<< CHANGE THIS to your data file path\n",
    "SEQUENCE_COL = \"Sequence\"                # <<< CHANGE THIS to your sequence column name\n",
    "TARGET_COL = \"DMS_score\"                 # <<< CHANGE THIS to your DMS score column name\n",
    "OUTPUT_DIR = \"./esm2_lora_finetuned_dms\"\n",
    "LOGGING_DIR = \"./logs\"\n",
    "TEST_SPLIT_SIZE = 0.15                   # Proportion of data for validation\n",
    "RANDOM_SEED = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['esm.encoder.layer.0.attention.self.value', 'esm.encoder.layer.0.attention.self.query', 'esm.encoder.layer.1.attention.self.value', 'esm.encoder.layer.1.attention.self.query', 'esm.encoder.layer.2.attention.self.value', 'esm.encoder.layer.2.attention.self.query', 'esm.encoder.layer.3.attention.self.value', 'esm.encoder.layer.3.attention.self.query', 'esm.encoder.layer.4.attention.self.value', 'esm.encoder.layer.4.attention.self.query', 'esm.encoder.layer.5.attention.self.value', 'esm.encoder.layer.5.attention.self.query', 'esm.encoder.layer.6.attention.self.value', 'esm.encoder.layer.6.attention.self.query', 'esm.encoder.layer.7.attention.self.value', 'esm.encoder.layer.7.attention.self.query', 'esm.encoder.layer.8.attention.self.value', 'esm.encoder.layer.8.attention.self.query', 'esm.encoder.layer.9.attention.self.value', 'esm.encoder.layer.9.attention.self.query', 'esm.encoder.layer.10.attention.self.value', 'esm.encoder.layer.10.attention.self.query', 'esm.encoder.layer.11.attention.self.value', 'esm.encoder.layer.11.attention.self.query']\n"
     ]
    }
   ],
   "source": [
    "# LoRA Configuration\n",
    "LORA_R = 8                               # LoRA rank (try 8, 16, 32)\n",
    "LORA_ALPHA = 16                          # LoRA alpha (often 2*r)\n",
    "LORA_DROPOUT = 0.1                       # LoRA dropout\n",
    "# --- Inspect your chosen model's architecture (`print(model)`) to find suitable target modules ---\n",
    "# For ESM models, attention query and value layers are common targets.\n",
    "# The exact names might vary slightly based on the specific ESM version in transformers.\n",
    "# Example for many ESM models:\n",
    "LORA_TARGET_MODULES = [\n",
    "    # \"esm.encoder.layer.*.attention.self.query\",\n",
    "    # \"esm.encoder.layer.*.attention.self.value\",\n",
    "    # Add other layers if needed, e.g., intermediate or output layers, but start simple.\n",
    "]\n",
    "for i in range(12):  # Assuming 12 layers (0-11)\n",
    "    LORA_TARGET_MODULES.append(f'esm.encoder.layer.{i}.attention.self.value')\n",
    "    LORA_TARGET_MODULES.append(f'esm.encoder.layer.{i}.attention.self.query')\n",
    "\n",
    "print(LORA_TARGET_MODULES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Configuration\n",
    "LEARNING_RATE = 1e-4 # Adjust as needed (can be higher than full fine-tuning)\n",
    "BATCH_SIZE = 8      # Adjust based on GPU memory\n",
    "NUM_EPOCHS = 10     # Adjust based on convergence (use early stopping)\n",
    "WEIGHT_DECAY = 0.01\n",
    "EVAL_STEPS = 50    # Evaluate every N steps\n",
    "SAVE_STEPS = 100   # Save checkpoint every N steps\n",
    "LOGGING_STEPS = 10\n",
    "EARLY_STOPPING_PATIENCE = 3 # Stop if validation metric doesn't improve for N evaluations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# --- Set Device ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from ../protein_embeddings.parquet...\n"
     ]
    }
   ],
   "source": [
    "# --- Load and Prepare Data ---\n",
    "print(f\"Loading data from {DATA_FILE}...\")\n",
    "df = pd.read_parquet(DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure target is numeric\n",
    "df[TARGET_COL] = pd.to_numeric(df[TARGET_COL])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded: 1140 samples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Basic validation\n",
    "if SEQUENCE_COL not in df.columns or TARGET_COL not in df.columns:\n",
    "    raise ValueError(f\"Columns '{SEQUENCE_COL}' or '{TARGET_COL}' not found in {DATA_FILE}\")\n",
    "\n",
    "print(f\"Data loaded: {len(df)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "train_df, val_df = train_test_split(\n",
    "    df, test_size=TEST_SPLIT_SIZE, random_state=RANDOM_SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 969\n",
      "Validation samples: 171\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Validation samples: {len(val_df)}\")\n",
    "\n",
    "# Convert pandas DataFrames to Hugging Face Datasets\n",
    "train_dataset = Dataset.from_pandas(train_df[[SEQUENCE_COL, TARGET_COL]].rename(columns={TARGET_COL: 'label', SEQUENCE_COL: 'text'}))\n",
    "val_dataset = Dataset.from_pandas(val_df[[SEQUENCE_COL, TARGET_COL]].rename(columns={TARGET_COL: 'label', SEQUENCE_COL: 'text'}))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer and Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer for facebook/esm2_t12_35M_UR50D...\n",
      "Loading base model facebook/esm2_t12_35M_UR50D for regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading tokenizer for {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(f\"Loading base model {MODEL_NAME} for regression...\")\n",
    "# We use AutoModelForSequenceClassification but configure it for regression\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=1,  # Single output value for regression\n",
    "    problem_type=\"regression\", # Ensures appropriate loss function (MSE)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "esm\n",
      "esm.embeddings\n",
      "esm.embeddings.word_embeddings\n",
      "esm.embeddings.dropout\n",
      "esm.embeddings.position_embeddings\n",
      "esm.encoder\n",
      "esm.encoder.layer\n",
      "esm.encoder.layer.0\n",
      "esm.encoder.layer.0.attention\n",
      "esm.encoder.layer.0.attention.self\n",
      "esm.encoder.layer.0.attention.self.query\n",
      "esm.encoder.layer.0.attention.self.key\n",
      "esm.encoder.layer.0.attention.self.value\n",
      "esm.encoder.layer.0.attention.self.dropout\n",
      "esm.encoder.layer.0.attention.self.rotary_embeddings\n",
      "esm.encoder.layer.0.attention.output\n",
      "esm.encoder.layer.0.attention.output.dense\n",
      "esm.encoder.layer.0.attention.output.dropout\n",
      "esm.encoder.layer.0.attention.LayerNorm\n",
      "esm.encoder.layer.0.intermediate\n",
      "esm.encoder.layer.0.intermediate.dense\n",
      "esm.encoder.layer.0.output\n",
      "esm.encoder.layer.0.output.dense\n",
      "esm.encoder.layer.0.output.dropout\n",
      "esm.encoder.layer.0.LayerNorm\n",
      "esm.encoder.layer.1\n",
      "esm.encoder.layer.1.attention\n",
      "esm.encoder.layer.1.attention.self\n",
      "esm.encoder.layer.1.attention.self.query\n",
      "esm.encoder.layer.1.attention.self.key\n",
      "esm.encoder.layer.1.attention.self.value\n",
      "esm.encoder.layer.1.attention.self.dropout\n",
      "esm.encoder.layer.1.attention.self.rotary_embeddings\n",
      "esm.encoder.layer.1.attention.output\n",
      "esm.encoder.layer.1.attention.output.dense\n",
      "esm.encoder.layer.1.attention.output.dropout\n",
      "esm.encoder.layer.1.attention.LayerNorm\n",
      "esm.encoder.layer.1.intermediate\n",
      "esm.encoder.layer.1.intermediate.dense\n",
      "esm.encoder.layer.1.output\n",
      "esm.encoder.layer.1.output.dense\n",
      "esm.encoder.layer.1.output.dropout\n",
      "esm.encoder.layer.1.LayerNorm\n",
      "esm.encoder.layer.2\n",
      "esm.encoder.layer.2.attention\n",
      "esm.encoder.layer.2.attention.self\n",
      "esm.encoder.layer.2.attention.self.query\n",
      "esm.encoder.layer.2.attention.self.key\n",
      "esm.encoder.layer.2.attention.self.value\n",
      "esm.encoder.layer.2.attention.self.dropout\n",
      "esm.encoder.layer.2.attention.self.rotary_embeddings\n",
      "esm.encoder.layer.2.attention.output\n",
      "esm.encoder.layer.2.attention.output.dense\n",
      "esm.encoder.layer.2.attention.output.dropout\n",
      "esm.encoder.layer.2.attention.LayerNorm\n",
      "esm.encoder.layer.2.intermediate\n",
      "esm.encoder.layer.2.intermediate.dense\n",
      "esm.encoder.layer.2.output\n",
      "esm.encoder.layer.2.output.dense\n",
      "esm.encoder.layer.2.output.dropout\n",
      "esm.encoder.layer.2.LayerNorm\n",
      "esm.encoder.layer.3\n",
      "esm.encoder.layer.3.attention\n",
      "esm.encoder.layer.3.attention.self\n",
      "esm.encoder.layer.3.attention.self.query\n",
      "esm.encoder.layer.3.attention.self.key\n",
      "esm.encoder.layer.3.attention.self.value\n",
      "esm.encoder.layer.3.attention.self.dropout\n",
      "esm.encoder.layer.3.attention.self.rotary_embeddings\n",
      "esm.encoder.layer.3.attention.output\n",
      "esm.encoder.layer.3.attention.output.dense\n",
      "esm.encoder.layer.3.attention.output.dropout\n",
      "esm.encoder.layer.3.attention.LayerNorm\n",
      "esm.encoder.layer.3.intermediate\n",
      "esm.encoder.layer.3.intermediate.dense\n",
      "esm.encoder.layer.3.output\n",
      "esm.encoder.layer.3.output.dense\n",
      "esm.encoder.layer.3.output.dropout\n",
      "esm.encoder.layer.3.LayerNorm\n",
      "esm.encoder.layer.4\n",
      "esm.encoder.layer.4.attention\n",
      "esm.encoder.layer.4.attention.self\n",
      "esm.encoder.layer.4.attention.self.query\n",
      "esm.encoder.layer.4.attention.self.key\n",
      "esm.encoder.layer.4.attention.self.value\n",
      "esm.encoder.layer.4.attention.self.dropout\n",
      "esm.encoder.layer.4.attention.self.rotary_embeddings\n",
      "esm.encoder.layer.4.attention.output\n",
      "esm.encoder.layer.4.attention.output.dense\n",
      "esm.encoder.layer.4.attention.output.dropout\n",
      "esm.encoder.layer.4.attention.LayerNorm\n",
      "esm.encoder.layer.4.intermediate\n",
      "esm.encoder.layer.4.intermediate.dense\n",
      "esm.encoder.layer.4.output\n",
      "esm.encoder.layer.4.output.dense\n",
      "esm.encoder.layer.4.output.dropout\n",
      "esm.encoder.layer.4.LayerNorm\n",
      "esm.encoder.layer.5\n",
      "esm.encoder.layer.5.attention\n",
      "esm.encoder.layer.5.attention.self\n",
      "esm.encoder.layer.5.attention.self.query\n",
      "esm.encoder.layer.5.attention.self.key\n",
      "esm.encoder.layer.5.attention.self.value\n",
      "esm.encoder.layer.5.attention.self.dropout\n",
      "esm.encoder.layer.5.attention.self.rotary_embeddings\n",
      "esm.encoder.layer.5.attention.output\n",
      "esm.encoder.layer.5.attention.output.dense\n",
      "esm.encoder.layer.5.attention.output.dropout\n",
      "esm.encoder.layer.5.attention.LayerNorm\n",
      "esm.encoder.layer.5.intermediate\n",
      "esm.encoder.layer.5.intermediate.dense\n",
      "esm.encoder.layer.5.output\n",
      "esm.encoder.layer.5.output.dense\n",
      "esm.encoder.layer.5.output.dropout\n",
      "esm.encoder.layer.5.LayerNorm\n",
      "esm.encoder.layer.6\n",
      "esm.encoder.layer.6.attention\n",
      "esm.encoder.layer.6.attention.self\n",
      "esm.encoder.layer.6.attention.self.query\n",
      "esm.encoder.layer.6.attention.self.key\n",
      "esm.encoder.layer.6.attention.self.value\n",
      "esm.encoder.layer.6.attention.self.dropout\n",
      "esm.encoder.layer.6.attention.self.rotary_embeddings\n",
      "esm.encoder.layer.6.attention.output\n",
      "esm.encoder.layer.6.attention.output.dense\n",
      "esm.encoder.layer.6.attention.output.dropout\n",
      "esm.encoder.layer.6.attention.LayerNorm\n",
      "esm.encoder.layer.6.intermediate\n",
      "esm.encoder.layer.6.intermediate.dense\n",
      "esm.encoder.layer.6.output\n",
      "esm.encoder.layer.6.output.dense\n",
      "esm.encoder.layer.6.output.dropout\n",
      "esm.encoder.layer.6.LayerNorm\n",
      "esm.encoder.layer.7\n",
      "esm.encoder.layer.7.attention\n",
      "esm.encoder.layer.7.attention.self\n",
      "esm.encoder.layer.7.attention.self.query\n",
      "esm.encoder.layer.7.attention.self.key\n",
      "esm.encoder.layer.7.attention.self.value\n",
      "esm.encoder.layer.7.attention.self.dropout\n",
      "esm.encoder.layer.7.attention.self.rotary_embeddings\n",
      "esm.encoder.layer.7.attention.output\n",
      "esm.encoder.layer.7.attention.output.dense\n",
      "esm.encoder.layer.7.attention.output.dropout\n",
      "esm.encoder.layer.7.attention.LayerNorm\n",
      "esm.encoder.layer.7.intermediate\n",
      "esm.encoder.layer.7.intermediate.dense\n",
      "esm.encoder.layer.7.output\n",
      "esm.encoder.layer.7.output.dense\n",
      "esm.encoder.layer.7.output.dropout\n",
      "esm.encoder.layer.7.LayerNorm\n",
      "esm.encoder.layer.8\n",
      "esm.encoder.layer.8.attention\n",
      "esm.encoder.layer.8.attention.self\n",
      "esm.encoder.layer.8.attention.self.query\n",
      "esm.encoder.layer.8.attention.self.key\n",
      "esm.encoder.layer.8.attention.self.value\n",
      "esm.encoder.layer.8.attention.self.dropout\n",
      "esm.encoder.layer.8.attention.self.rotary_embeddings\n",
      "esm.encoder.layer.8.attention.output\n",
      "esm.encoder.layer.8.attention.output.dense\n",
      "esm.encoder.layer.8.attention.output.dropout\n",
      "esm.encoder.layer.8.attention.LayerNorm\n",
      "esm.encoder.layer.8.intermediate\n",
      "esm.encoder.layer.8.intermediate.dense\n",
      "esm.encoder.layer.8.output\n",
      "esm.encoder.layer.8.output.dense\n",
      "esm.encoder.layer.8.output.dropout\n",
      "esm.encoder.layer.8.LayerNorm\n",
      "esm.encoder.layer.9\n",
      "esm.encoder.layer.9.attention\n",
      "esm.encoder.layer.9.attention.self\n",
      "esm.encoder.layer.9.attention.self.query\n",
      "esm.encoder.layer.9.attention.self.key\n",
      "esm.encoder.layer.9.attention.self.value\n",
      "esm.encoder.layer.9.attention.self.dropout\n",
      "esm.encoder.layer.9.attention.self.rotary_embeddings\n",
      "esm.encoder.layer.9.attention.output\n",
      "esm.encoder.layer.9.attention.output.dense\n",
      "esm.encoder.layer.9.attention.output.dropout\n",
      "esm.encoder.layer.9.attention.LayerNorm\n",
      "esm.encoder.layer.9.intermediate\n",
      "esm.encoder.layer.9.intermediate.dense\n",
      "esm.encoder.layer.9.output\n",
      "esm.encoder.layer.9.output.dense\n",
      "esm.encoder.layer.9.output.dropout\n",
      "esm.encoder.layer.9.LayerNorm\n",
      "esm.encoder.layer.10\n",
      "esm.encoder.layer.10.attention\n",
      "esm.encoder.layer.10.attention.self\n",
      "esm.encoder.layer.10.attention.self.query\n",
      "esm.encoder.layer.10.attention.self.key\n",
      "esm.encoder.layer.10.attention.self.value\n",
      "esm.encoder.layer.10.attention.self.dropout\n",
      "esm.encoder.layer.10.attention.self.rotary_embeddings\n",
      "esm.encoder.layer.10.attention.output\n",
      "esm.encoder.layer.10.attention.output.dense\n",
      "esm.encoder.layer.10.attention.output.dropout\n",
      "esm.encoder.layer.10.attention.LayerNorm\n",
      "esm.encoder.layer.10.intermediate\n",
      "esm.encoder.layer.10.intermediate.dense\n",
      "esm.encoder.layer.10.output\n",
      "esm.encoder.layer.10.output.dense\n",
      "esm.encoder.layer.10.output.dropout\n",
      "esm.encoder.layer.10.LayerNorm\n",
      "esm.encoder.layer.11\n",
      "esm.encoder.layer.11.attention\n",
      "esm.encoder.layer.11.attention.self\n",
      "esm.encoder.layer.11.attention.self.query\n",
      "esm.encoder.layer.11.attention.self.key\n",
      "esm.encoder.layer.11.attention.self.value\n",
      "esm.encoder.layer.11.attention.self.dropout\n",
      "esm.encoder.layer.11.attention.self.rotary_embeddings\n",
      "esm.encoder.layer.11.attention.output\n",
      "esm.encoder.layer.11.attention.output.dense\n",
      "esm.encoder.layer.11.attention.output.dropout\n",
      "esm.encoder.layer.11.attention.LayerNorm\n",
      "esm.encoder.layer.11.intermediate\n",
      "esm.encoder.layer.11.intermediate.dense\n",
      "esm.encoder.layer.11.output\n",
      "esm.encoder.layer.11.output.dense\n",
      "esm.encoder.layer.11.output.dropout\n",
      "esm.encoder.layer.11.LayerNorm\n",
      "esm.encoder.emb_layer_norm_after\n",
      "esm.contact_head\n",
      "esm.contact_head.regression\n",
      "esm.contact_head.activation\n",
      "classifier\n",
      "classifier.dense\n",
      "classifier.dropout\n",
      "classifier.out_proj\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 969/969 [00:01<00:00, 493.93 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 171/171 [00:00<00:00, 486.78 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    # Ensure sequences are strings\n",
    "    sequences = [str(seq) for seq in examples[\"text\"]]\n",
    "    # Truncation is important for models with fixed input size like ESM\n",
    "    return tokenizer(sequences, padding=\"max_length\", truncation=True, max_length=1024) # Adjust max_length if needed, but ESM models often have a limit\n",
    "\n",
    "print(\"Tokenizing datasets...\")\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Remove unnecessary columns\n",
    "tokenized_train_dataset = tokenized_train_dataset.remove_columns([\"text\", \"__index_level_0__\"])\n",
    "tokenized_val_dataset = tokenized_val_dataset.remove_columns([\"text\", \"__index_level_0__\"])\n",
    "tokenized_train_dataset.set_format(\"torch\")\n",
    "tokenized_val_dataset.set_format(\"torch\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring LoRA...\n"
     ]
    }
   ],
   "source": [
    "print(\"Configuring LoRA...\")\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS, # Use SEQ_CLS even for regression with this model head\n",
    "    inference_mode=False,\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    target_modules=LORA_TARGET_MODULES,\n",
    "    # bias=\"none\" # or \"all\" or \"lora_only\". Check peft docs. 'none' often works well.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA model configured:\n",
      "trainable params: 415,681 || all params: 34,409,043 || trainable%: 1.2081\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Wrap the base model with PEFT adapter\n",
    "model = get_peft_model(model, peft_config)\n",
    "print(\"LoRA model configured:\")\n",
    "model.print_trainable_parameters() # Should show a very small % of trainable parameters\n",
    "\n",
    "model = model.to(device) # Move PEFT model to GPU if available\n",
    "print(model.device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Metrics ---\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    # Logits are the direct output for regression\n",
    "    predictions = logits.squeeze(-1) # Remove the last dimension if necessary\n",
    "    # labels = labels.squeeze(-1)\n",
    "\n",
    "    # Calculate Pearson Correlation\n",
    "    pearson_corr, p_value = pearsonr(predictions, labels)\n",
    "\n",
    "    # Calculate Mean Squared Error (optional but good to track)\n",
    "    mse = ((predictions - labels) ** 2).mean().item()\n",
    "\n",
    "    return {\n",
    "        \"pearsonr\": pearson_corr,\n",
    "        \"mse\": mse,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Training Arguments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/rmehta98/.conda/envs/compbio/lib/python3.13/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(\"Setting up Training Arguments...\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    evaluation_strategy=\"steps\", # Evaluate periodically\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    save_strategy=\"steps\",       # Save periodically\n",
    "    save_steps=SAVE_STEPS,\n",
    "    logging_dir=LOGGING_DIR,\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    load_best_model_at_end=True, # Load the best checkpoint at the end of training\n",
    "    metric_for_best_model=\"pearsonr\", # Use Pearson correlation to determine the best model\n",
    "    greater_is_better=True,      # Higher Pearson correlation is better\n",
    "    save_total_limit=2,          # Only keep the best and the latest checkpoint\n",
    "    fp16=torch.cuda.is_available(), # Enable mixed precision if GPU is available\n",
    "    report_to=\"tensorboard\",     # Log metrics for visualization\n",
    "    seed=RANDOM_SEED,\n",
    "    # dataloader_num_workers=4, # Optional: Speed up data loading if I/O is bottleneck\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator handles padding within batches dynamically\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Trainer\n",
    "from typing import Dict, Union, Any, Optional, Tuple, List\n",
    "import logging # Using logging is better for debug messages\n",
    "\n",
    "# Configure basic logging (optional, but helpful for debugging)\n",
    "# logging.basicConfig(level=logging.DEBUG) # Uncomment this line to enable debug prints\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Compute loss, explicitly handling potential dict-like loss output\n",
    "        and unexpected keyword arguments like 'num_items_in_batch'.\n",
    "        \"\"\"\n",
    "        # Make a copy to avoid modifying the original input dict if labels are popped\n",
    "        inputs_copy = inputs.copy()\n",
    "        labels = inputs_copy.pop(\"labels\", None)\n",
    "\n",
    "        # Ensure labels are on the correct device if model is on GPU and labels exist\n",
    "        if labels is not None and hasattr(model, 'device'):\n",
    "            labels = labels.to(model.device)\n",
    "\n",
    "        # logger.debug(f\"Inputs passed to model: {list(inputs_copy.keys())}\")\n",
    "        # logger.debug(f\"Labels present: {labels is not None}\")\n",
    "        if labels is not None:\n",
    "            # logger.debug(f\"Labels tensor device: {labels.device}\")\n",
    "            pass\n",
    "\n",
    "        # Forward pass using the modified inputs (without labels if they were popped)\n",
    "        outputs = model(**inputs_copy)\n",
    "        # logger.debug(f\"Model output type: {type(outputs)}\")\n",
    "        # logger.debug(f\"Model outputs: {outputs}\") # Caution: Can be very verbose\n",
    "\n",
    "        loss = None # Initialize loss\n",
    "\n",
    "        # Check if the model computed the loss automatically\n",
    "        if hasattr(outputs, \"loss\") and outputs.loss is not None:\n",
    "            loss_val = outputs.loss # Get the value from the outputs object\n",
    "            # logger.debug(f\"Type of outputs.loss: {type(loss_val)}\")\n",
    "            # logger.debug(f\"Value of outputs.loss: {loss_val}\")\n",
    "\n",
    "            # --- Critical Check ---\n",
    "            # Check if the extracted loss is actually a tensor\n",
    "            if isinstance(loss_val, torch.Tensor):\n",
    "                loss = loss_val\n",
    "            # --- Handle if loss_val is dict ---\n",
    "            elif isinstance(loss_val, dict):\n",
    "                # If outputs.loss is unexpectedly a dict, try common keys\n",
    "                if 'loss' in loss_val and isinstance(loss_val['loss'], torch.Tensor):\n",
    "                     loss = loss_val['loss']\n",
    "                     logger.warning(\"outputs.loss was a dict, extracted loss tensor from key 'loss'. Check model output structure.\")\n",
    "                # Add other potential keys if you inspect outputs and find loss elsewhere\n",
    "                # elif 'some_other_key' in loss_val ...\n",
    "                else:\n",
    "                    # If we can't find a tensor in the dict, raise an error\n",
    "                    raise TypeError(f\"outputs.loss is a dictionary but does not contain a recognizable loss tensor under expected keys: {loss_val}\")\n",
    "            else:\n",
    "                 # Handle other unexpected types\n",
    "                 raise TypeError(f\"outputs.loss has an unexpected type: {type(loss_val)}. Expected torch.Tensor or dict containing tensor.\")\n",
    "\n",
    "        # If we successfully extracted a valid loss tensor, ensure it's a scalar\n",
    "        if loss is not None:\n",
    "            # logger.debug(f\"Extracted Loss Tensor: {loss}, Shape: {loss.shape}, Dim: {loss.dim()}\")\n",
    "            # Ensure it's a tensor before calling .dim() - redundant now due to checks above, but safe\n",
    "            if isinstance(loss, torch.Tensor):\n",
    "                if loss.dim() != 0:\n",
    "                    # This often happens in multi-GPU (DistributedDataParallel) scenarios\n",
    "                    # where the loss might be calculated per device or per sample initially.\n",
    "                    # Averaging across the batch dimension is usually correct.\n",
    "                    logger.warning(f\"Loss tensor has dim {loss.dim()} (expected 0). Averaging loss.\")\n",
    "                    loss = loss.mean()\n",
    "                    # logger.debug(f\"Averaged Loss: {loss}\")\n",
    "            else:\n",
    "                 # This should not happen due to the checks above, but as a safeguard:\n",
    "                 raise TypeError(f\"Variable 'loss' was expected to be a Tensor but got {type(loss)} before dimension check.\")\n",
    "\n",
    "        # Handle cases where loss couldn't be determined (e.g., no labels provided during eval)\n",
    "        elif not return_outputs:\n",
    "             # During training, loss should always be computable if labels are provided.\n",
    "             # If loss is None here during training, something is wrong upstream.\n",
    "             # During evaluation without labels, loss will be None, which is okay.\n",
    "             logger.warning(\"compute_loss: Loss is None (e.g., no labels provided or model didn't return loss), and return_outputs=False.\")\n",
    "             # What should happen here? The default Trainer might expect a Tensor.\n",
    "             # Returning None might cause issues downstream in the training loop.\n",
    "             # Let's raise an error if loss is required (i.e., labels were present initially)\n",
    "             if labels is not None:\n",
    "                 raise ValueError(\"Labels were provided, but loss could not be computed or extracted from model outputs.\")\n",
    "             # If no labels provided (eval mode), returning None for loss might be acceptable depending on Trainer internals\n",
    "             # or how evaluation loop handles it. Let's stick to returning None in this specific eval case.\n",
    "\n",
    "        # else: loss is None, but return_outputs is True (likely evaluation), which is fine.\n",
    "\n",
    "        # logger.debug(f\"Final computed loss value: {loss}\")\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_827997/3307582636.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# --- Initialize Trainer ---\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    # data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=EARLY_STOPPING_PATIENCE)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='1220' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   3/1220 00:00 < 05:37, 3.60 it/s, Epoch 0.02/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =      4.918\n",
      "  total_flos               =   925679GF\n",
      "  train_loss               =     0.0429\n",
      "  train_runtime            = 0:03:17.10\n",
      "  train_samples_per_second =     49.162\n",
      "  train_steps_per_second   =       6.19\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "# --- Train the Model ---\n",
    "print(\"Starting training...\")\n",
    "train_result = trainer.train()\n",
    "\n",
    "# Save training metrics\n",
    "metrics = train_result.metrics\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the best model on the validation set...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='22' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2/22 00:00 < 00:02, 8.92 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Results:\n",
      "{'eval_loss': 0.048407647758722305, 'eval_pearsonr': 0.12966124713420868, 'eval_mse': 0.048407647758722305, 'eval_runtime': 2.3744, 'eval_samples_per_second': 72.019, 'eval_steps_per_second': 9.266, 'epoch': 4.918032786885246}\n",
      "***** eval metrics *****\n",
      "  epoch                   =      4.918\n",
      "  eval_loss               =     0.0484\n",
      "  eval_mse                =     0.0484\n",
      "  eval_pearsonr           =     0.1297\n",
      "  eval_runtime            = 0:00:02.37\n",
      "  eval_samples_per_second =     72.019\n",
      "  eval_steps_per_second   =      9.266\n"
     ]
    }
   ],
   "source": [
    "# --- Evaluate the Best Model ---\n",
    "print(\"Evaluating the best model on the validation set...\")\n",
    "eval_results = trainer.evaluate(eval_dataset=tokenized_val_dataset)\n",
    "\n",
    "print(\"Validation Results:\")\n",
    "print(eval_results)\n",
    "trainer.log_metrics(\"eval\", eval_results)\n",
    "trainer.save_metrics(\"eval\", eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final PEFT adapter saved to ./esm2_lora_finetuned_dms/final_adapter\n"
     ]
    }
   ],
   "source": [
    "# --- Save the final PEFT adapter ---\n",
    "# The best model checkpoint is already saved by `load_best_model_at_end=True`\n",
    "# You can optionally save the adapter explicitly if needed\n",
    "final_adapter_path = os.path.join(OUTPUT_DIR, \"final_adapter\")\n",
    "model.save_pretrained(final_adapter_path)\n",
    "tokenizer.save_pretrained(final_adapter_path)\n",
    "print(f\"Final PEFT adapter saved to {final_adapter_path}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example Inference ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): EsmForSequenceClassification(\n",
       "      (esm): EsmModel(\n",
       "        (embeddings): EsmEmbeddings(\n",
       "          (word_embeddings): Embedding(33, 480, padding_idx=1)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (position_embeddings): Embedding(1026, 480, padding_idx=1)\n",
       "        )\n",
       "        (encoder): EsmEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0-11): 12 x EsmLayer(\n",
       "              (attention): EsmAttention(\n",
       "                (self): EsmSelfAttention(\n",
       "                  (query): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=480, out_features=480, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=480, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=480, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=480, out_features=480, bias=True)\n",
       "                  (value): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=480, out_features=480, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=480, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=480, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (rotary_embeddings): RotaryEmbedding()\n",
       "                )\n",
       "                (output): EsmSelfOutput(\n",
       "                  (dense): Linear(in_features=480, out_features=480, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (intermediate): EsmIntermediate(\n",
       "                (dense): Linear(in_features=480, out_features=1920, bias=True)\n",
       "              )\n",
       "              (output): EsmOutput(\n",
       "                (dense): Linear(in_features=1920, out_features=480, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (emb_layer_norm_after): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (contact_head): EsmContactPredictionHead(\n",
       "          (regression): Linear(in_features=240, out_features=1, bias=True)\n",
       "          (activation): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (classifier): ModulesToSaveWrapper(\n",
       "        (original_module): EsmClassificationHead(\n",
       "          (dense): Linear(in_features=480, out_features=480, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (out_proj): Linear(in_features=480, out_features=1, bias=True)\n",
       "        )\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): EsmClassificationHead(\n",
       "            (dense): Linear(in_features=480, out_features=480, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (out_proj): Linear(in_features=480, out_features=1, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n--- Example Inference ---\")\n",
    "# Load the base model again\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=1,\n",
    "    problem_type=\"regression\",\n",
    ")\n",
    "\n",
    "# Load the PEFT adapter weights on top of the base model\n",
    "inference_model = PeftModel.from_pretrained(base_model, final_adapter_path)\n",
    "inference_model = inference_model.to(device)\n",
    "inference_model.eval() # Set to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DMS score for: MVNLARGNSSLNPCLEGSASSGSESSKDSS...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example new sequence (replace with actual sequences you want to predict)\n",
    "new_sequence = \"MVNLARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLREKMRRRLESGDKWFSLEFFPPRTAEGAVNLISRFDRMAAGGPLYIDVTWHPAGDPGSDKETSSMMIASTAVNYCGLETILHMTCCRQRLEEITGHLHKAKQLGLKNIMALRGDPIGDQWEEEEGGFNYAVDLVKHIRSEFGDYFDICVAGYPKGHPEAGSFEADLKHLKEKVSAGADFIITQLFFEADTFFRFVKACTDMGITCPIVPGIFPIQGYHSLRQLVKLSKLEVPQEIKDVIEPIKDNDAAIRNYGIELAVSLCQELLASGLVPGLHFYTLNREMATTEVLKRLGMWTEDPRRPLPWALSAHPKRREEDVRPIFWASRPKSYIYRTQEWDEFPNGRWGNSSSPAFGELKDYYLFYLKSKSPKEELLKMWGEELTSEESVFEVFVLYLSGEPNRNGHKVTCLPWNDEPLAAETSLLKEELLRVNRQGILTINSQPNINGKPSSDPIVGWGPSGGYVFQKAYLEFFTSRETAEALLQVLKKYELRVNYHLVNVKGENITNAPELQPNAVTWGIFPGREIIQPTVVDPVSFMFWKDEAFALWIERWGKLYEEESPSRTIIQYIHDNYFLVNLVDNDFPLDNCLWQVVEDTLELLNRPTQNARETEAP\" # A protein sequence string (use one from your data or a hypothetical one)\n",
    "print(f\"Predicting DMS score for: {new_sequence[:30]}...\") # Print start of sequence\n",
    "\n",
    "# Tokenize the new sequence\n",
    "inputs = tokenizer(new_sequence, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024)\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()} # Move inputs to device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted DMS Score: 0.2048\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Make prediction\n",
    "with torch.no_grad():\n",
    "    outputs = inference_model(**inputs)\n",
    "    predicted_dms_score = outputs.logits.item() # Get the single regression value\n",
    "\n",
    "print(f\"Predicted DMS Score: {predicted_dms_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compbio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
