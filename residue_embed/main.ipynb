{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../train.csv')\n",
    "\n",
    "# get the sequence\n",
    "seq = open('../sequence.fasta', 'r').read()\n",
    "seq = seq.split(\"\\n\")[1]\n",
    "\n",
    "# create each mutated sequence using the info\n",
    "sequences = []\n",
    "indices = []\n",
    "for i in df['mutant']:\n",
    "    ind = int(i[1:-1])\n",
    "    tmp = seq[:ind] + i[-1] + seq[ind+1:]\n",
    "    sequences.append(tmp)\n",
    "    indices.append(ind)\n",
    "df['Sequence'] = sequences\n",
    "df['Position'] = indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mutant</th>\n",
       "      <th>DMS_score</th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Position</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M0Y</td>\n",
       "      <td>0.2730</td>\n",
       "      <td>YVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M0W</td>\n",
       "      <td>0.2857</td>\n",
       "      <td>WVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M0V</td>\n",
       "      <td>0.2153</td>\n",
       "      <td>VVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M0T</td>\n",
       "      <td>0.3122</td>\n",
       "      <td>TVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M0S</td>\n",
       "      <td>0.2180</td>\n",
       "      <td>SVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1135</th>\n",
       "      <td>P347D</td>\n",
       "      <td>0.3876</td>\n",
       "      <td>MVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...</td>\n",
       "      <td>347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1136</th>\n",
       "      <td>P347C</td>\n",
       "      <td>0.1837</td>\n",
       "      <td>MVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...</td>\n",
       "      <td>347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1137</th>\n",
       "      <td>P347A</td>\n",
       "      <td>0.4611</td>\n",
       "      <td>MVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...</td>\n",
       "      <td>347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1138</th>\n",
       "      <td>P347M</td>\n",
       "      <td>0.2412</td>\n",
       "      <td>MVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...</td>\n",
       "      <td>347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1139</th>\n",
       "      <td>P347H</td>\n",
       "      <td>0.1512</td>\n",
       "      <td>MVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...</td>\n",
       "      <td>347</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1140 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mutant  DMS_score                                           Sequence  \\\n",
       "0       M0Y     0.2730  YVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...   \n",
       "1       M0W     0.2857  WVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...   \n",
       "2       M0V     0.2153  VVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...   \n",
       "3       M0T     0.3122  TVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...   \n",
       "4       M0S     0.2180  SVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...   \n",
       "...     ...        ...                                                ...   \n",
       "1135  P347D     0.3876  MVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...   \n",
       "1136  P347C     0.1837  MVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...   \n",
       "1137  P347A     0.4611  MVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...   \n",
       "1138  P347M     0.2412  MVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...   \n",
       "1139  P347H     0.1512  MVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...   \n",
       "\n",
       "      Position  \n",
       "0            0  \n",
       "1            0  \n",
       "2            0  \n",
       "3            0  \n",
       "4            0  \n",
       "...        ...  \n",
       "1135       347  \n",
       "1136       347  \n",
       "1137       347  \n",
       "1138       347  \n",
       "1139       347  \n",
       "\n",
       "[1140 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t6_8M_UR50D.pt\" to /home/hice1/smamidipaka3/.cache/torch/hub/checkpoints/esm2_t6_8M_UR50D.pt\n",
      "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/regression/esm2_t6_8M_UR50D-contact-regression.pt\" to /home/hice1/smamidipaka3/.cache/torch/hub/checkpoints/esm2_t6_8M_UR50D-contact-regression.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity at position 1: 0.8984\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import esm\n",
    "\n",
    "# Load ESM-2 model (esm2_t6_8M is small and fast, scale up later if needed)\n",
    "model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "model.eval()  # disable dropout for eval mode\n",
    "\n",
    "# Example wild-type and mutant sequences\n",
    "\n",
    "wild_type_sequence = seq\n",
    "mutant_sequence = df.loc[1]['Sequence']\n",
    "\n",
    "# Prepare data (must be a list of (name, sequence) tuples)\n",
    "data = [(\"wt\", wild_type_sequence), (\"mut\", mutant_sequence)]\n",
    "batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "\n",
    "with torch.no_grad():\n",
    "    results = model(batch_tokens, repr_layers=[6], return_contacts=False)\n",
    "\n",
    "# Extract per-residue embeddings from the desired layer (layer 6 here)\n",
    "# Shape: [batch_size, sequence_length, embedding_dim]\n",
    "token_representations = results[\"representations\"][6]\n",
    "\n",
    "# Get the per-residue embeddings for WT and mutant (excluding padding and start/end tokens)\n",
    "wt_embedding = token_representations[0, 1:-1]  # shape: [L, D]\n",
    "mut_embedding = token_representations[1, 1:-1]  # shape: [L, D]\n",
    "\n",
    "# Example: get embedding at mutation position (e.g., position 20 → index 19)\n",
    "mutation_pos = 0\n",
    "wt_residue_vec = wt_embedding[mutation_pos]\n",
    "mut_residue_vec = mut_embedding[mutation_pos]\n",
    "\n",
    "# Cosine similarity between WT and mutant residue embedding\n",
    "cos_sim = torch.nn.functional.cosine_similarity(wt_residue_vec, mut_residue_vec, dim=0)\n",
    "print(f\"Cosine similarity at position {mutation_pos + 1}: {cos_sim.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import esm\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pyarrow.parquet as pq  # Make sure pyarrow is installed\n",
    "\n",
    "\n",
    "def get_embed(df: pd.DataFrame, output_file: str, include_fitness=False):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load ESM-2 model\n",
    "    model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "    model = model.to(device)\n",
    "\n",
    "    batch_converter = alphabet.get_batch_converter()\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize list to hold data\n",
    "    data_rows = []\n",
    "\n",
    "    # Loop through all rows in the mutation DataFrame\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        try:\n",
    "            mutant_sequence = row[\"Sequence\"]\n",
    "            mutation_pos = int(row[\"Position\"])  # 0-based index\n",
    "            if include_fitness:\n",
    "                fitness_score = float(row[\"DMS_score\"])\n",
    "\n",
    "            # Prepare batch for ESM\n",
    "            batch_data = [(\"wt\", seq), (\"mut\", mutant_sequence)]\n",
    "            _, _, batch_tokens = batch_converter(batch_data)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                results = model(batch_tokens.to(device), repr_layers=[6], return_contacts=False)\n",
    "\n",
    "            reps = results[\"representations\"][6]\n",
    "            wt_embedding = reps[0, 1:-1]\n",
    "            mut_embedding = reps[1, 1:-1]\n",
    "\n",
    "            # Skip if mutation position is invalid\n",
    "            if mutation_pos >= wt_embedding.shape[0]:\n",
    "                continue\n",
    "\n",
    "            # Extract residue embeddings\n",
    "            wt_vec = wt_embedding[mutation_pos]\n",
    "            mut_vec = mut_embedding[mutation_pos]\n",
    "\n",
    "            # Combine features (concat + delta)\n",
    "            feature_vec = torch.cat((wt_vec, mut_vec), dim=0).cpu().numpy()\n",
    "\n",
    "            # Store in list\n",
    "            if include_fitness:\n",
    "                data_rows.append({\n",
    "                    \"features\": feature_vec,\n",
    "                    \"fitness\": fitness_score\n",
    "                })\n",
    "            else:\n",
    "                data_rows.append({\n",
    "                    \"features\": feature_vec\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error on row {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    features_df = pd.DataFrame(data_rows)\n",
    "    # Expand the feature vectors into columns\n",
    "    features_df = features_df.join(pd.DataFrame(features_df.pop(\"features\").tolist()))\n",
    "\n",
    "    # Save as parquet\n",
    "    features_df.to_parquet(output_file, index=False)\n",
    "    print(f\"Saved to {output_file}.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 320])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wt_embedding[17:21].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss = 0.9982, Val Loss = 1.0228\n",
      "Epoch 02: Train Loss = 0.9174, Val Loss = 0.9166\n",
      "Epoch 03: Train Loss = 0.8534, Val Loss = 0.8788\n",
      "Epoch 04: Train Loss = 0.8071, Val Loss = 0.9741\n",
      "Epoch 05: Train Loss = 0.7985, Val Loss = 0.8807\n",
      "Epoch 06: Train Loss = 0.7534, Val Loss = 0.9049\n",
      "Epoch 07: Train Loss = 0.6870, Val Loss = 0.8995\n",
      "Epoch 08: Train Loss = 0.6491, Val Loss = 0.9158\n",
      "Epoch 09: Train Loss = 0.6219, Val Loss = 0.9328\n",
      "Epoch 10: Train Loss = 0.5727, Val Loss = 0.9443\n",
      "Epoch 11: Train Loss = 0.5058, Val Loss = 0.9127\n",
      "Epoch 12: Train Loss = 0.4521, Val Loss = 1.0039\n",
      "Epoch 13: Train Loss = 0.3960, Val Loss = 0.9196\n",
      "Epoch 14: Train Loss = 0.3843, Val Loss = 1.0565\n",
      "Epoch 15: Train Loss = 0.3490, Val Loss = 1.0067\n",
      "Epoch 16: Train Loss = 0.2960, Val Loss = 0.9825\n",
      "Epoch 17: Train Loss = 0.2967, Val Loss = 1.1807\n",
      "Epoch 18: Train Loss = 0.3241, Val Loss = 0.9597\n",
      "Epoch 19: Train Loss = 0.2773, Val Loss = 1.0464\n",
      "Epoch 20: Train Loss = 0.2332, Val Loss = 0.9860\n",
      "Epoch 21: Train Loss = 0.2305, Val Loss = 1.0020\n",
      "Epoch 22: Train Loss = 0.1678, Val Loss = 0.9221\n",
      "Epoch 23: Train Loss = 0.1371, Val Loss = 1.0292\n",
      "Epoch 24: Train Loss = 0.1113, Val Loss = 0.9741\n",
      "Epoch 25: Train Loss = 0.1210, Val Loss = 1.1036\n",
      "Epoch 26: Train Loss = 0.1109, Val Loss = 0.9280\n",
      "Epoch 27: Train Loss = 0.1016, Val Loss = 1.0496\n",
      "Epoch 28: Train Loss = 0.0965, Val Loss = 0.9810\n",
      "Epoch 29: Train Loss = 0.0793, Val Loss = 1.0443\n",
      "Epoch 30: Train Loss = 0.0664, Val Loss = 0.9239\n",
      "Epoch 31: Train Loss = 0.0575, Val Loss = 1.0588\n",
      "Epoch 32: Train Loss = 0.0668, Val Loss = 0.9697\n",
      "Epoch 33: Train Loss = 0.0824, Val Loss = 1.1258\n",
      "Epoch 34: Train Loss = 0.0964, Val Loss = 0.9901\n",
      "Epoch 35: Train Loss = 0.0848, Val Loss = 0.9668\n",
      "Epoch 36: Train Loss = 0.0866, Val Loss = 1.0307\n",
      "Epoch 37: Train Loss = 0.0735, Val Loss = 1.0332\n",
      "Epoch 38: Train Loss = 0.0725, Val Loss = 0.9944\n",
      "Epoch 39: Train Loss = 0.0592, Val Loss = 1.0288\n",
      "Epoch 40: Train Loss = 0.0722, Val Loss = 0.9539\n",
      "Epoch 41: Train Loss = 0.0573, Val Loss = 1.0260\n",
      "Epoch 42: Train Loss = 0.0504, Val Loss = 0.9829\n",
      "Epoch 43: Train Loss = 0.0512, Val Loss = 0.9689\n",
      "Epoch 44: Train Loss = 0.0434, Val Loss = 0.9815\n",
      "Epoch 45: Train Loss = 0.0450, Val Loss = 0.9495\n",
      "Epoch 46: Train Loss = 0.0483, Val Loss = 0.9576\n",
      "Epoch 47: Train Loss = 0.0362, Val Loss = 1.0066\n",
      "Epoch 48: Train Loss = 0.0323, Val Loss = 0.9628\n",
      "Epoch 49: Train Loss = 0.0332, Val Loss = 1.0543\n",
      "Epoch 50: Train Loss = 0.0297, Val Loss = 0.9847\n",
      "\n",
      "🧪 Test MSE: 0.0324\n",
      "📈 R² Score: 0.1802\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_parquet(\"protein_mutation_fitness.parquet\")\n",
    "\n",
    "# Extract X (features) and y (fitness score)\n",
    "X = df.drop(columns=[\"fitness\"]).values.astype(np.float32)\n",
    "y = df[\"fitness\"].values.astype(np.float32).reshape(-1, 1)\n",
    "\n",
    "# Normalize features\n",
    "scaler_X = StandardScaler()\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "y_scaled = scaler_y.fit_transform(y)\n",
    "\n",
    "# Convert to tensors\n",
    "X_tensor = torch.tensor(X_scaled)\n",
    "y_tensor = torch.tensor(y_scaled)\n",
    "\n",
    "# Create dataset\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "# Train/val/test split\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_set, val_set, test_set = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=64)\n",
    "test_loader = DataLoader(test_set, batch_size=64)\n",
    "\n",
    "# Define MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MLP(input_dim=X.shape[1]).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        pred = model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * len(xb)\n",
    "\n",
    "    val_loss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            pred = model(xb)\n",
    "            val_loss += criterion(pred, yb).item() * len(xb)\n",
    "\n",
    "    print(f\"Epoch {epoch+1:02d}: Train Loss = {total_loss / train_size:.4f}, Val Loss = {val_loss / val_size:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "model.eval()\n",
    "y_preds = []\n",
    "y_true = []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(device)\n",
    "        preds = model(xb).cpu()\n",
    "        y_preds.append(preds)\n",
    "        y_true.append(yb)\n",
    "\n",
    "y_preds = torch.cat(y_preds).numpy()\n",
    "y_true = torch.cat(y_true).numpy()\n",
    "\n",
    "# Unscale predictions\n",
    "y_preds = scaler_y.inverse_transform(y_preds)\n",
    "y_true = scaler_y.inverse_transform(y_true)\n",
    "\n",
    "mse = mean_squared_error(y_true, y_preds)\n",
    "r2 = r2_score(y_true, y_preds)\n",
    "print(f\"\\n🧪 Test MSE: {mse:.4f}\")\n",
    "print(f\"📈 R² Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\tvalid_0's rmse: 0.206362\n",
      "[20]\tvalid_0's rmse: 0.199665\n",
      "[30]\tvalid_0's rmse: 0.197501\n",
      "[40]\tvalid_0's rmse: 0.195936\n",
      "[50]\tvalid_0's rmse: 0.195923\n",
      "Early stopping, best iteration is:\n",
      "[46]\tvalid_0's rmse: 0.195436\n",
      "\n",
      "✅ RMSE: 0.1954\n",
      "📈 Spearman correlation: 0.4907\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import spearmanr\n",
    "import numpy as np\n",
    "\n",
    "# Split your dataset\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y.ravel(), test_size=0.2, random_state=42)\n",
    "\n",
    "# Create LightGBM datasets\n",
    "train_set = lgb.Dataset(X_train, label=y_train)\n",
    "val_set = lgb.Dataset(X_val, label=y_val)\n",
    "\n",
    "# Define parameters for regression\n",
    "params = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"metric\": \"rmse\",\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"num_leaves\": 64,\n",
    "    \"verbosity\": -1\n",
    "}\n",
    "\n",
    "# Train the model with early stopping\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_set,\n",
    "    valid_sets=[val_set],\n",
    "    num_boost_round=1000,\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=10), lgb.log_evaluation(period=10)]\n",
    ")\n",
    "\n",
    "# Predict on validation set\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# Evaluation\n",
    "mse = mean_squared_error(y_val, y_pred)\n",
    "spearman = spearmanr(y_val, y_pred).correlation\n",
    "\n",
    "print(f\"\\n✅ RMSE: {np.sqrt(mse):.4f}\")\n",
    "print(f\"📈 Spearman correlation: {spearman:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('../test.csv')\n",
    "\n",
    "sequences = []\n",
    "indices = []\n",
    "for i in df_test['mutant']:\n",
    "    ind = int(i[1:-1])\n",
    "    tmp = seq[:ind] + i[-1] + seq[ind+1:]\n",
    "    sequences.append(tmp)\n",
    "    indices.append(ind)\n",
    "df_test['Sequence'] = sequences\n",
    "df_test['Position'] = indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t33_650M_UR50D.pt\" to /home/hice1/smamidipaka3/.cache/torch/hub/checkpoints/esm2_t33_650M_UR50D.pt\n",
      "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/regression/esm2_t33_650M_UR50D-contact-regression.pt\" to /home/hice1/smamidipaka3/.cache/torch/hub/checkpoints/esm2_t33_650M_UR50D-contact-regression.pt\n",
      "100%|██████████| 11324/11324 [37:21<00:00,  5.05it/s]\n",
      "/tmp/ipykernel_340502/2508642839.py:69: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  features_df = features_df.join(pd.DataFrame(features_df.pop(\"features\").tolist()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to protein_mutation_fitness_test.parquet.parquet\n"
     ]
    }
   ],
   "source": [
    "get_embed(df_test,'protein_mutation_fitness_test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compbiohack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
